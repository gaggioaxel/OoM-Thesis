{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def custom_header_reader(file:'TextIOWrapper'):\n",
    "    csvReader = csv.reader(file,delimiter='\\t')\n",
    "    for row, text in enumerate(csvReader):\n",
    "        if row == 2: numMarkers = int(text[-1])\n",
    "        elif row == 10: \n",
    "            columnNames = text[:-1]\n",
    "            if columnNames[0] == 'Frame': \n",
    "                break\n",
    "        elif row == 11: columnNames = text[:-1]; break\n",
    "    return numMarkers, columnNames, csvReader\n",
    "\n",
    "def line_reader(csvReader,fromSecond,toSecond):\n",
    "    for line in csvReader:\n",
    "        if fromSecond <= float(line[1]) <= toSecond:\n",
    "            yield line[:len(columnNames)]\n",
    "        elif float(line[1]) > toSecond:\n",
    "            break\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "ANNOTATIONS_PATH = 'data'\n",
    "RAW_PATH = ANNOTATIONS_PATH+'/raw'\n",
    "#with open(ANNOTATIONS_PATH+'/annotations.txt','r') as file:\n",
    "#    annotations = file.read().splitlines()\n",
    "with open(ANNOTATIONS_PATH+'/annotationsVReduced.txt','r') as file:\n",
    "    annotations = file.read().splitlines()[1:]#[line for line in file.read().splitlines() if 'no val' in line]\n",
    "    #print(len(annotations))\n",
    "    \n",
    "for sampleAnno in range(-len(annotations)):\n",
    "    if '(A)' in annotations[sampleAnno] or '(B)' in annotations[sampleAnno]:\n",
    "        folder, trial, fragId, side, OoM, startSec, endSec = annotations[sampleAnno].replace(' ','').split(',')[:7]\n",
    "    else:\n",
    "        folder, trial, fragId, OoM, startSec, endSec = annotations[sampleAnno].replace(' ','').split(',')[:6]\n",
    "    sampleData = os.path.join(RAW_PATH,folder,trial+'.tsv')\n",
    "    #print(sampleData,startSec,endSec)\n",
    "    with open(sampleData,'r') as file:\n",
    "        numMarkers, columnNames, readerBuffer = custom_header_reader(file)\n",
    "        data = pd.DataFrame(line_reader(readerBuffer,float(startSec),float(endSec)),columns=columnNames).astype(dict(zip(columnNames,[int,float,str]+[float]*len(columnNames[3:]))))\n",
    "    print(sampleAnno,end='\\r')\n",
    "    #print(data['Time'])\n",
    "    #print(sampleAnno, data['Time'].iloc[0])\n",
    "    if data['Time'].iloc[0] > float(startSec) or data['Time'].iloc[-1] < float(endSec):\n",
    "        raise Exception(f'\\x1b[31mNOT FULL FRAMES!!! data starts at: {data[\"Time\"].iloc[0]} of {float(startSec)} and ends at {data[\"Time\"].iloc[-1]} of {float(endSec)}\\x1b[0m')\n",
    "    else:\n",
    "        #print(\"\\x1b[32mSTART AND END SECONDS CHECK CORRECT!\\x1b[0m\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    if '_A' in data.columns[3] or '_B' in data.columns[3]:\n",
    "        data = data[['Frame','Time','SMPTE']+data.filter(like=f'_{side[1]}').columns.tolist()]\n",
    "        data.columns = [col.replace(f'_{side[1]}','') for col in data.columns]\n",
    "    elif '61A' in data.columns[3] or '61B' in data.columns[3]:\n",
    "        data = data[['Frame','Time','SMPTE']+data.filter(like=f'61{side[1]}').columns.tolist()]\n",
    "        data.columns = [col.replace(f'61{side[1]}_','') for col in data.columns]\n",
    "    if '_01' in data.columns[3]:\n",
    "        data.columns = [column for column in data.columns[:3]]+[column[:-5]+column[-2:] for column in data.columns[3:]]\n",
    "    if 'HeadFront' in data.columns[3] or 'RKneeIn' in data.columns[3]:\n",
    "        mappings = pd.read_csv(\"data/raw/ExtendedMarkersToStandard.csv\")\n",
    "        mappings = mappings.set_index(mappings.columns[0])[mappings.columns[1]].to_dict()\n",
    "        data.columns = [mappings[col[:-2]]+col[-2:] if col[:-2] in mappings.keys() else col for col in data.columns]\n",
    "\n",
    "\n",
    "    #print(\"Total NaNs\")\n",
    "    nans = data[data.columns[(data.isna().sum(axis=0) > 0 ).to_list()]].isna().sum(axis=0).sort_values(ascending=False)\n",
    "    color = '\\x1b[32m' if len(nans) == 0 else '\\x1b[31m'\n",
    "    #print(color+str(nans)+'\\x1b[0m '+ ('\\nNo Nans' if len(nans)==0 else '\\nNans found!'))\n",
    "\n",
    "    from itertools import groupby\n",
    "    max_nans_per_column = data.apply(lambda col: max(len(list(group))*value for value, group in groupby(col.isna().tolist())))\n",
    "    #print(\"Max contiguous NaNs\")\n",
    "    max_nans_per_column = max_nans_per_column[max_nans_per_column > 0].sort_values(ascending=False)\n",
    "    color = '\\x1b[32m' if len(max_nans_per_column) == 0 else '\\x1b[31m'\n",
    "    #print(color+str(max_nans_per_column)+'\\x1b[0m ' + ('\\nNo Nans' if len(nans)==0 else '\\nNans found!'))\n",
    "\n",
    "    #print('Num Zeros')\n",
    "    zeros = data[data.columns[(data.eq(0.0).sum(axis=0) > 0).to_list()]].eq(0.0).sum(axis=0).sort_values(ascending=False)\n",
    "    color = '\\x1b[32m' if len(zeros) == 0 else '\\x1b[31m'\n",
    "    #print(color+str(zeros)+'\\x1b[0m ' + ('\\nNo Zeros' if len(zeros)==0 else '\\nZeros found!'))\n",
    "    #list(zeros.index)\n",
    "\n",
    "    # THIS IS A FIX FOR MISSING QTM AT DATA 58-59\n",
    "    import numpy as np\n",
    "    if len(zeros.index) > 15:\n",
    "        for name_col,num_zeros in zeros.items():\n",
    "            if num_zeros < 50:\n",
    "                data[name_col] = data[name_col].replace(0.0,np.nan).interpolate(method='spline', order=3)\n",
    "\n",
    "\n",
    "    from itertools import groupby\n",
    "    max_zeros_per_column = data.apply(lambda col: max(len(list(group))*value for value, group in groupby(col.eq(0.0).tolist())))\n",
    "    #print(\"Max contiguous Zeros\")\n",
    "    max_zeros_per_column = max_zeros_per_column[max_zeros_per_column > 0].sort_values(ascending=False)\n",
    "    color = '\\x1b[32m' if len(max_zeros_per_column) == 0 else '\\x1b[31m'\n",
    "    #print(color+str(max_zeros_per_column)+'\\x1b[0m ' + ('\\nNo Zeros' if len(max_zeros_per_column)==0 else '\\nZeros found!'))\n",
    "\n",
    "\n",
    "    def map_reduce_num_markers(reducedMarkerNames:list):\n",
    "        if not 'ARIEL' in fullMarkerNames:\n",
    "            pass\n",
    "        markersMap = {reducedMarkerNames[0]:   ['LHEL','LMT1','LMT5','LTOE'],\n",
    "                      reducedMarkerNames[1]:   ['RHEL','RMT1','RMT5','RTOE'],\n",
    "                      reducedMarkerNames[2]:   ['LANK'],\n",
    "                      reducedMarkerNames[3]:   ['RANK'],\n",
    "                      reducedMarkerNames[4]:   ['LKNE','LKNI'],\n",
    "                      reducedMarkerNames[5]:   ['RKNE','RKNI'],\n",
    "                      reducedMarkerNames[6]:   ['LFWT','LBWT'],\n",
    "                      reducedMarkerNames[7]:   ['RFWT','LFWT','RBWT','LBWT'],\n",
    "                      reducedMarkerNames[8]:   ['RFWT','RBWT'],\n",
    "                      reducedMarkerNames[9]:   ['C6','C7','T5','T10','BWT','STRN','CLAV','FWT'],\n",
    "                      reducedMarkerNames[10]:  ['LPLM','LTHMB','LINDX','LMID','LPNKY'],\n",
    "                      reducedMarkerNames[11]:  ['RPLM','RTHMB','RINDX','RMID','RPNKY'],\n",
    "                      reducedMarkerNames[12]:  ['LOWR','LIWR'],\n",
    "                      reducedMarkerNames[13]:  ['ROWR','RIWR'],\n",
    "                      reducedMarkerNames[14]:  ['LELB','LIEL','LFRM'],\n",
    "                      reducedMarkerNames[15]:  ['RELB','RIEL','RFRM'],\n",
    "                      reducedMarkerNames[16]:  ['LSHO'],\n",
    "                      reducedMarkerNames[17]:  ['RSHO','LSHO'],\n",
    "                      reducedMarkerNames[18]:  ['RSHO'],\n",
    "                      reducedMarkerNames[19]:  ['ARIEL','RFHD','LFHD','RBHD','LBHD']\n",
    "                      }\n",
    "        #   removing nan joints\n",
    "        for nan_joint_name,_ in max_nans_per_column[::3].items():\n",
    "            nan_joint_name = nan_joint_name[:-2]\n",
    "            for markerFullList in markersMap.values():\n",
    "                if nan_joint_name in markerFullList:\n",
    "                    markerFullList.remove(nan_joint_name)\n",
    "                    break\n",
    "        # removing many contiguous zeros joints\n",
    "        for zero_joint_name,contiguous_zeros in max_zeros_per_column[::3].items():\n",
    "            if contiguous_zeros > 20:\n",
    "                zero_joint_name = zero_joint_name[:-2]\n",
    "                for markerFullList in markersMap.values():\n",
    "                    if zero_joint_name in markerFullList:\n",
    "                        markerFullList.remove(zero_joint_name)\n",
    "                        break\n",
    "        originalDataColumns = [colname[:-2] for colname in list(data.iloc[:1,3::3].columns)]\n",
    "\n",
    "        #   removing markers not present in the data markers\n",
    "        for markerFullList in markersMap.values():\n",
    "            for markerName in list(reversed(markerFullList)):\n",
    "                if not markerName in originalDataColumns:\n",
    "                    markerFullList.remove(markerName)\n",
    "\n",
    "        #   removing asymmetries across body\n",
    "        for left_side_marker_indx,right_side_marker_indx in [(0,1),(4,5),(6,8),(10,11),(12,13),(14,15)]: # indices of every marker on the left side and his symmetric\n",
    "            markersLeft = markersMap[reducedMarkerNames[left_side_marker_indx]]\n",
    "            markersRight = markersMap[reducedMarkerNames[right_side_marker_indx]]\n",
    "            for elem in markersLeft:\n",
    "                if not 'R'+elem[1:] in markersRight:\n",
    "                    markersLeft.remove(elem)\n",
    "                    if left_side_marker_indx == 6: \n",
    "                        markersMap[reducedMarkerNames[7]].remove(elem)\n",
    "            for elem in markersRight:\n",
    "                if not 'L'+elem[1:] in markersLeft:\n",
    "                    markersRight.remove(elem)\n",
    "                    if left_side_marker_indx == 6: \n",
    "                        markersMap[reducedMarkerNames[7]].remove(elem)\n",
    "\n",
    "        for i,markerFullList in enumerate(markersMap.values()):\n",
    "            if len(markerFullList) == 0:\n",
    "                print(markersMap)\n",
    "                raise Exception(f\"can't reconstruct '{reducedMarkerNames[i]}' of 20 markers, because every marker of the full set is missing!!\")\n",
    "        return markersMap\n",
    "\n",
    "\n",
    "    reducedMarkerNames = ['left_foot',      # 0\n",
    "                          'right_foot',     # 1          \n",
    "                          'left_ank',       # 2      \n",
    "                          'right_ank',      # 3                          \n",
    "                          'left_knee',      # 4                          \n",
    "                          'right_knee',     # 5                              \n",
    "                          'left_hip',       # 6                          \n",
    "                          'hip_central',    # 7                              \n",
    "                          'right_hip',      # 8                          \n",
    "                          'spine',          # 9                       \n",
    "                          'left_hand',      # 10                           \n",
    "                          'right_hand',     # 11                               \n",
    "                          'left_wrist',     # 12                               \n",
    "                          'right_wrist',    # 13                               \n",
    "                          'left_elbow',     # 14                               \n",
    "                          'right_elbow',    # 15                               \n",
    "                          'left_shoulder',  # 16                               \n",
    "                          'shoulder_center',# 17                                   \n",
    "                          'right_shoulder', # 18                                   \n",
    "                          'head']           # 19                       \n",
    "    fullMarkerNames = [colname[:-2] for colname in list(data.iloc[:1,3::3].columns)]\n",
    "    reduced20MarkersToFullMarkers = map_reduce_num_markers(reducedMarkerNames)\n",
    "    reduced20MarkersToFullMarkersX = {key+'_X': [elem+' X' for elem in reduced20MarkersToFullMarkers[key]] for key in reduced20MarkersToFullMarkers.keys()}\n",
    "    reduced20MarkersToFullMarkersY = {key+'_Y': [elem+' Y' for elem in reduced20MarkersToFullMarkers[key]] for key in reduced20MarkersToFullMarkers.keys()}\n",
    "    reduced20MarkersToFullMarkersZ = {key+'_Z': [elem+' Z' for elem in reduced20MarkersToFullMarkers[key]] for key in reduced20MarkersToFullMarkers.keys()}\n",
    "\n",
    "    posTable = data.iloc[:,3:]\n",
    "\n",
    "    posTableX = posTable.iloc[:,::3]\n",
    "    posTableY = posTable.iloc[:,1::3]\n",
    "    posTableZ = posTable.iloc[:,2::3]\n",
    "\n",
    "    reducedPosTableX = pd.concat([posTableX[reduced20MarkersToFullMarkersX[colName+'_X']].mean(axis=1) for colName in reducedMarkerNames],axis=1,keys=[name+'_X' for name in reducedMarkerNames])\n",
    "    reducedPosTableY = pd.concat([posTableY[reduced20MarkersToFullMarkersY[colName+'_Y']].mean(axis=1) for colName in reducedMarkerNames],axis=1,keys=[name+'_Y' for name in reducedMarkerNames])\n",
    "    reducedPosTableZ = pd.concat([posTableZ[reduced20MarkersToFullMarkersZ[colName+'_Z']].mean(axis=1) for colName in reducedMarkerNames],axis=1,keys=[name+'_Z' for name in reducedMarkerNames])\n",
    "\n",
    "    #reducedPosTableX = (reducedPosTableX - reducedPosTableX.min(axis=None))/(reducedPosTableX.max(axis=None)-reducedPosTableX.min(axis=None))\n",
    "    #reducedPosTableY = (reducedPosTableY - reducedPosTableY.min(axis=None))/(reducedPosTableY.max(axis=None)-reducedPosTableY.min(axis=None))\n",
    "    #reducedPosTableZ = (reducedPosTableZ - reducedPosTableZ.min(axis=None))/(reducedPosTableZ.max(axis=None)-reducedPosTableZ.min(axis=None))\n",
    "\n",
    "    def xyz_tables_to_xyz_columns(tablesList):\n",
    "        xTable,yTable,zTable = tablesList\n",
    "        mergedTable = pd.DataFrame()\n",
    "        for joint in range(xTable.shape[1]):\n",
    "            mergedTable = pd.concat([mergedTable,xTable.iloc[:,joint],yTable.iloc[:,joint],zTable.iloc[:,joint]],axis=1)\n",
    "        return mergedTable\n",
    "\n",
    "    reducedPosTable = xyz_tables_to_xyz_columns([reducedPosTableX,reducedPosTableY,reducedPosTableZ])\n",
    "    reducedPosTable = (reducedPosTable - reducedPosTable.min(axis=None))/(reducedPosTable.max(axis=None)-reducedPosTable.min(axis=None))\n",
    "    reducedPosTable.head()\n",
    "\n",
    "    import os\n",
    "\n",
    "    PROCESSED_PATH = 'data/reprocessed/'\n",
    "\n",
    "    dest_dir = os.path.join(PROCESSED_PATH,folder)\n",
    "    if not os.path.exists(dest_dir): \n",
    "        os.mkdir(dest_dir)\n",
    "    dest_file_path = os.path.join(dest_dir,trial+'_frag'+fragId+'.csv')\n",
    "    if not os.path.isfile(dest_file_path) or True:\n",
    "        reducedPosTable.to_csv(dest_file_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_foot_X</th>\n",
       "      <th>left_foot_Y</th>\n",
       "      <th>left_foot_Z</th>\n",
       "      <th>right_foot_X</th>\n",
       "      <th>right_foot_Y</th>\n",
       "      <th>right_foot_Z</th>\n",
       "      <th>left_ank_X</th>\n",
       "      <th>left_ank_Y</th>\n",
       "      <th>left_ank_Z</th>\n",
       "      <th>right_ank_X</th>\n",
       "      <th>...</th>\n",
       "      <th>left_shoulder_Z</th>\n",
       "      <th>shoulder_center_X</th>\n",
       "      <th>shoulder_center_Y</th>\n",
       "      <th>shoulder_center_Z</th>\n",
       "      <th>right_shoulder_X</th>\n",
       "      <th>right_shoulder_Y</th>\n",
       "      <th>right_shoulder_Z</th>\n",
       "      <th>head_X</th>\n",
       "      <th>head_Y</th>\n",
       "      <th>head_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.599149</td>\n",
       "      <td>0.282866</td>\n",
       "      <td>0.281366</td>\n",
       "      <td>0.603479</td>\n",
       "      <td>0.282036</td>\n",
       "      <td>0.196394</td>\n",
       "      <td>0.626746</td>\n",
       "      <td>0.309066</td>\n",
       "      <td>0.291376</td>\n",
       "      <td>0.634263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310926</td>\n",
       "      <td>0.610581</td>\n",
       "      <td>0.753321</td>\n",
       "      <td>0.244585</td>\n",
       "      <td>0.613030</td>\n",
       "      <td>0.757138</td>\n",
       "      <td>0.178245</td>\n",
       "      <td>0.620697</td>\n",
       "      <td>0.841934</td>\n",
       "      <td>0.245584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599149</td>\n",
       "      <td>0.282867</td>\n",
       "      <td>0.281366</td>\n",
       "      <td>0.603481</td>\n",
       "      <td>0.282036</td>\n",
       "      <td>0.196393</td>\n",
       "      <td>0.626744</td>\n",
       "      <td>0.309068</td>\n",
       "      <td>0.291374</td>\n",
       "      <td>0.634298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310933</td>\n",
       "      <td>0.610508</td>\n",
       "      <td>0.753349</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.612879</td>\n",
       "      <td>0.757172</td>\n",
       "      <td>0.178267</td>\n",
       "      <td>0.620695</td>\n",
       "      <td>0.841926</td>\n",
       "      <td>0.245566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.599149</td>\n",
       "      <td>0.282866</td>\n",
       "      <td>0.281365</td>\n",
       "      <td>0.603482</td>\n",
       "      <td>0.282037</td>\n",
       "      <td>0.196392</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.309063</td>\n",
       "      <td>0.291373</td>\n",
       "      <td>0.634445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310944</td>\n",
       "      <td>0.610425</td>\n",
       "      <td>0.753376</td>\n",
       "      <td>0.244616</td>\n",
       "      <td>0.612703</td>\n",
       "      <td>0.757201</td>\n",
       "      <td>0.178288</td>\n",
       "      <td>0.620691</td>\n",
       "      <td>0.841918</td>\n",
       "      <td>0.245548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599152</td>\n",
       "      <td>0.282863</td>\n",
       "      <td>0.281364</td>\n",
       "      <td>0.603482</td>\n",
       "      <td>0.282036</td>\n",
       "      <td>0.196392</td>\n",
       "      <td>0.626735</td>\n",
       "      <td>0.309062</td>\n",
       "      <td>0.291373</td>\n",
       "      <td>0.633893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310954</td>\n",
       "      <td>0.610346</td>\n",
       "      <td>0.753399</td>\n",
       "      <td>0.244627</td>\n",
       "      <td>0.612542</td>\n",
       "      <td>0.757220</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.620685</td>\n",
       "      <td>0.841909</td>\n",
       "      <td>0.245532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599150</td>\n",
       "      <td>0.282868</td>\n",
       "      <td>0.281363</td>\n",
       "      <td>0.603481</td>\n",
       "      <td>0.282040</td>\n",
       "      <td>0.196393</td>\n",
       "      <td>0.626724</td>\n",
       "      <td>0.309069</td>\n",
       "      <td>0.291376</td>\n",
       "      <td>0.634368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310969</td>\n",
       "      <td>0.610267</td>\n",
       "      <td>0.753423</td>\n",
       "      <td>0.244642</td>\n",
       "      <td>0.612376</td>\n",
       "      <td>0.757248</td>\n",
       "      <td>0.178315</td>\n",
       "      <td>0.620681</td>\n",
       "      <td>0.841904</td>\n",
       "      <td>0.245516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   left_foot_X  left_foot_Y  left_foot_Z  right_foot_X  right_foot_Y  \\\n",
       "0     0.599149     0.282866     0.281366      0.603479      0.282036   \n",
       "1     0.599149     0.282867     0.281366      0.603481      0.282036   \n",
       "2     0.599149     0.282866     0.281365      0.603482      0.282037   \n",
       "3     0.599152     0.282863     0.281364      0.603482      0.282036   \n",
       "4     0.599150     0.282868     0.281363      0.603481      0.282040   \n",
       "\n",
       "   right_foot_Z  left_ank_X  left_ank_Y  left_ank_Z  right_ank_X  ...  \\\n",
       "0      0.196394    0.626746    0.309066    0.291376     0.634263  ...   \n",
       "1      0.196393    0.626744    0.309068    0.291374     0.634298  ...   \n",
       "2      0.196392    0.626739    0.309063    0.291373     0.634445  ...   \n",
       "3      0.196392    0.626735    0.309062    0.291373     0.633893  ...   \n",
       "4      0.196393    0.626724    0.309069    0.291376     0.634368  ...   \n",
       "\n",
       "   left_shoulder_Z  shoulder_center_X  shoulder_center_Y  shoulder_center_Z  \\\n",
       "0         0.310926           0.610581           0.753321           0.244585   \n",
       "1         0.310933           0.610508           0.753349           0.244600   \n",
       "2         0.310944           0.610425           0.753376           0.244616   \n",
       "3         0.310954           0.610346           0.753399           0.244627   \n",
       "4         0.310969           0.610267           0.753423           0.244642   \n",
       "\n",
       "   right_shoulder_X  right_shoulder_Y  right_shoulder_Z    head_X    head_Y  \\\n",
       "0          0.613030          0.757138          0.178245  0.620697  0.841934   \n",
       "1          0.612879          0.757172          0.178267  0.620695  0.841926   \n",
       "2          0.612703          0.757201          0.178288  0.620691  0.841918   \n",
       "3          0.612542          0.757220          0.178300  0.620685  0.841909   \n",
       "4          0.612376          0.757248          0.178315  0.620681  0.841904   \n",
       "\n",
       "     head_Z  \n",
       "0  0.245584  \n",
       "1  0.245566  \n",
       "2  0.245548  \n",
       "3  0.245532  \n",
       "4  0.245516  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ANNOTATIONS_PATH = 'data'\n",
    "REPROCESSED_PATH = ANNOTATIONS_PATH+'/reprocessed'\n",
    "with open(ANNOTATIONS_PATH+'/annotationsVSingle.txt','r') as file:\n",
    "    annotations = [line for line in file.read().splitlines()[1:]]\n",
    "\n",
    "sampleAnno = 0\n",
    "def read_data(id: int):\n",
    "    if '(A)' in annotations[id] or '(B)' in annotations[id]:\n",
    "        folder, trial, fragId, _, OoM = annotations[id].replace(' ','').split(',')[:5]\n",
    "    else:\n",
    "        folder, trial, fragId, OoM = annotations[id].replace(' ','').split(',')[:4]\n",
    "    file = os.path.join(REPROCESSED_PATH, folder, trial+'_frag'+fragId+'.csv')\n",
    "    table = pd.read_csv(file)\n",
    "    return table, OoM\n",
    "dataTable,_ = read_data(sampleAnno)\n",
    "dataTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_list_xyz_tables(table:pd.DataFrame,into=\"xyz\"):\n",
    "    if into == \"xyz\":\n",
    "        return table.iloc[:,::3],table.iloc[:,1::3],table.iloc[:,2::3]\n",
    "    elif into == \"points\":\n",
    "        return [table.iloc[:,j:j+3] for j in range(0,table.shape[1],3)]\n",
    "    \n",
    "def group_table_by_joints(table: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns = [col.replace('_X','') for col in list(table.columns)[::3]]\n",
    "    result = pd.DataFrame(columns=columns)\n",
    "    for j in range(0,table.shape[1],3):\n",
    "        lst = []\n",
    "        for i in range(table.shape[0]):\n",
    "            lst.append(table.iloc[i,j:j+3].values)\n",
    "        result[columns[int(j/3)]] = lst\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.stats import iqr\n",
    "\n",
    "def extractFeatures(features):\n",
    "  raw_features = features.shape[1]\n",
    "  arr = features\n",
    "  energy = np.sqrt(np.sum(arr ** 2, axis=0))\n",
    "  fft = np.fft.fft(arr, axis=0)\n",
    "  amplitude_spectrum = np.abs(fft)\n",
    "  phase_angle = np.angle(fft) \n",
    "\n",
    "  frq_info = [\n",
    "              phase_angle[0, :],\n",
    "              np.mean(fft.real, axis=0),\n",
    "              np.max(fft.real, axis=0),\n",
    "              np.argmax(fft.real, axis=0),\n",
    "              np.min(fft.real, axis=0),\n",
    "              np.argmin(fft.real, axis=0),\n",
    "              skew(amplitude_spectrum, axis=0, bias=True),\n",
    "              kurtosis(amplitude_spectrum, axis=0, bias=True),\n",
    "  ] \n",
    "\n",
    "  frq_info = np.hstack(frq_info)\n",
    "  mean = np.mean(arr, axis=0)\n",
    "  var = np.var(arr, axis=0)\n",
    "  kurt = kurtosis(arr, axis=0, bias=True)\n",
    "  skew_ = skew(arr, axis=0, bias=True)\n",
    "  corr = np.corrcoef(arr, rowvar=False)[np.triu_indices(raw_features,k=1)]\n",
    "  mad = np.mean(np.abs(arr - mean), axis=0)\n",
    "  sem = np.std(arr, axis=0) / np.sqrt(arr.shape[0])\n",
    "  mi = np.min(arr, axis=0)\n",
    "  ma = np.max(arr, axis=0)\n",
    "  return np.hstack([mean,var,kurt,skew_,corr,mad,sem,energy,iqr(arr,axis=0),mi,ma,frq_info])\n",
    "\n",
    "def get_feature_names(raw_features):#df, metadata):\n",
    "  #df_raw_feat = df.columns\n",
    "  new_names = [] #new_names = list(df_raw_feat[:metadata])\n",
    "  #raw_features = df_raw_feat[metadata:]\n",
    "\n",
    "  # Define the feature names based on the order they appear in extractFeatures\n",
    "  new_names += [f\"mean_{i}\" for i in raw_features]\n",
    "  new_names += [f\"var_{i}\" for i in raw_features]\n",
    "  new_names += [f\"kurt_{i}\" for i in raw_features]\n",
    "  new_names += [f\"skew_{i}\" for i in raw_features]\n",
    "  new_names += [f\"corr_{i_name}_{j_name}\" for i, i_name in enumerate(raw_features[:-1]) for j, j_name in enumerate(raw_features[i+1:])]\n",
    "  new_names += [f\"mad_{i}\" for i in raw_features]\n",
    "  new_names += [f\"sem_{i}\" for i in raw_features]\n",
    "  new_names += [f\"energy_{i}\" for i in raw_features]\n",
    "  new_names += [f\"iqr_{i}\" for i in raw_features]\n",
    "  new_names += [f\"min_{i}\" for i in raw_features]\n",
    "  new_names += [f\"max_{i}\" for i in raw_features]\n",
    "\n",
    "  new_names += [f\"fft_phase_angle_{i}\" for i in raw_features]\n",
    "  new_names += [f\"fft_mean_real{i}\" for i in raw_features]\n",
    "  new_names += [f\"fft_max_real{i}\" for i in raw_features]\n",
    "  new_names += [f\"fft_argmax_real{i}\" for i in raw_features]\n",
    "  new_names += [f\"fft_min_real{i}\" for i in raw_features]\n",
    "  new_names += [f\"fft_argmin_real{i}\" for i in raw_features]\n",
    "  new_names += [f\"fft_skew_amp_spec{i}\" for i in raw_features]\n",
    "  new_names += [f\"fft_kurt_amp_spec{i}\" for i in raw_features]\n",
    "\n",
    "  return new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 3): 0,\n",
       " (2, 4): 1,\n",
       " (3, 5): 2,\n",
       " (5, 7): 3,\n",
       " (6, 9): 4,\n",
       " (7, 8): 5,\n",
       " (8, 9): 6,\n",
       " (11, 13): 7,\n",
       " (12, 14): 8,\n",
       " (13, 15): 9,\n",
       " (15, 17): 10,\n",
       " (16, 19): 11,\n",
       " (17, 18): 12,\n",
       " (18, 19): 13,\n",
       " (18, 20): 14}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OoM_set = set([tuple(map(int, read_data(i)[1].strip('[]').split('-'))) for i in range(len(annotations))])\n",
    "OoM_to_ord = dict(zip(sorted(list(OoM_set)),range(len(annotations))))\n",
    "ord_to_OoM = dict(zip(range(len(annotations)),sorted(list(OoM_set))))\n",
    "OoM_to_ord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works on single labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing features for the dataset: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_left_foot</th>\n",
       "      <th>mean_right_foot</th>\n",
       "      <th>mean_left_ank</th>\n",
       "      <th>mean_right_ank</th>\n",
       "      <th>mean_left_knee</th>\n",
       "      <th>mean_right_knee</th>\n",
       "      <th>mean_left_hip</th>\n",
       "      <th>mean_hip_central</th>\n",
       "      <th>mean_right_hip</th>\n",
       "      <th>mean_spine</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_kurt_amp_specright_hand</th>\n",
       "      <th>fft_kurt_amp_specleft_wrist</th>\n",
       "      <th>fft_kurt_amp_specright_wrist</th>\n",
       "      <th>fft_kurt_amp_specleft_elbow</th>\n",
       "      <th>fft_kurt_amp_specright_elbow</th>\n",
       "      <th>fft_kurt_amp_specleft_shoulder</th>\n",
       "      <th>fft_kurt_amp_specshoulder_center</th>\n",
       "      <th>fft_kurt_amp_specright_shoulder</th>\n",
       "      <th>fft_kurt_amp_spechead</th>\n",
       "      <th>OoM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_left_foot mean_right_foot mean_left_ank mean_right_ank mean_left_knee  \\\n",
       "0             NaN             NaN           NaN            NaN            NaN   \n",
       "1             NaN             NaN           NaN            NaN            NaN   \n",
       "2             NaN             NaN           NaN            NaN            NaN   \n",
       "3             NaN             NaN           NaN            NaN            NaN   \n",
       "4             NaN             NaN           NaN            NaN            NaN   \n",
       "5             NaN             NaN           NaN            NaN            NaN   \n",
       "6             NaN             NaN           NaN            NaN            NaN   \n",
       "7             NaN             NaN           NaN            NaN            NaN   \n",
       "8             NaN             NaN           NaN            NaN            NaN   \n",
       "9             NaN             NaN           NaN            NaN            NaN   \n",
       "10            NaN             NaN           NaN            NaN            NaN   \n",
       "11            NaN             NaN           NaN            NaN            NaN   \n",
       "12            NaN             NaN           NaN            NaN            NaN   \n",
       "13            NaN             NaN           NaN            NaN            NaN   \n",
       "14            NaN             NaN           NaN            NaN            NaN   \n",
       "15            NaN             NaN           NaN            NaN            NaN   \n",
       "16            NaN             NaN           NaN            NaN            NaN   \n",
       "17            NaN             NaN           NaN            NaN            NaN   \n",
       "18            NaN             NaN           NaN            NaN            NaN   \n",
       "19            NaN             NaN           NaN            NaN            NaN   \n",
       "20            NaN             NaN           NaN            NaN            NaN   \n",
       "21            NaN             NaN           NaN            NaN            NaN   \n",
       "22            NaN             NaN           NaN            NaN            NaN   \n",
       "23            NaN             NaN           NaN            NaN            NaN   \n",
       "24            NaN             NaN           NaN            NaN            NaN   \n",
       "25            NaN             NaN           NaN            NaN            NaN   \n",
       "26            NaN             NaN           NaN            NaN            NaN   \n",
       "27            NaN             NaN           NaN            NaN            NaN   \n",
       "28            NaN             NaN           NaN            NaN            NaN   \n",
       "29            NaN             NaN           NaN            NaN            NaN   \n",
       "30            NaN             NaN           NaN            NaN            NaN   \n",
       "31            NaN             NaN           NaN            NaN            NaN   \n",
       "32            NaN             NaN           NaN            NaN            NaN   \n",
       "33            NaN             NaN           NaN            NaN            NaN   \n",
       "34            NaN             NaN           NaN            NaN            NaN   \n",
       "35            NaN             NaN           NaN            NaN            NaN   \n",
       "36            NaN             NaN           NaN            NaN            NaN   \n",
       "37            NaN             NaN           NaN            NaN            NaN   \n",
       "38            NaN             NaN           NaN            NaN            NaN   \n",
       "39            NaN             NaN           NaN            NaN            NaN   \n",
       "40            NaN             NaN           NaN            NaN            NaN   \n",
       "41            NaN             NaN           NaN            NaN            NaN   \n",
       "42            NaN             NaN           NaN            NaN            NaN   \n",
       "43            NaN             NaN           NaN            NaN            NaN   \n",
       "44            NaN             NaN           NaN            NaN            NaN   \n",
       "45            NaN             NaN           NaN            NaN            NaN   \n",
       "46            NaN             NaN           NaN            NaN            NaN   \n",
       "47            NaN             NaN           NaN            NaN            NaN   \n",
       "48            NaN             NaN           NaN            NaN            NaN   \n",
       "49            NaN             NaN           NaN            NaN            NaN   \n",
       "50            NaN             NaN           NaN            NaN            NaN   \n",
       "51            NaN             NaN           NaN            NaN            NaN   \n",
       "52            NaN             NaN           NaN            NaN            NaN   \n",
       "53            NaN             NaN           NaN            NaN            NaN   \n",
       "54            NaN             NaN           NaN            NaN            NaN   \n",
       "55            NaN             NaN           NaN            NaN            NaN   \n",
       "56            NaN             NaN           NaN            NaN            NaN   \n",
       "57            NaN             NaN           NaN            NaN            NaN   \n",
       "58            NaN             NaN           NaN            NaN            NaN   \n",
       "59            NaN             NaN           NaN            NaN            NaN   \n",
       "\n",
       "   mean_right_knee mean_left_hip mean_hip_central mean_right_hip mean_spine  \\\n",
       "0              NaN           NaN              NaN            NaN        NaN   \n",
       "1              NaN           NaN              NaN            NaN        NaN   \n",
       "2              NaN           NaN              NaN            NaN        NaN   \n",
       "3              NaN           NaN              NaN            NaN        NaN   \n",
       "4              NaN           NaN              NaN            NaN        NaN   \n",
       "5              NaN           NaN              NaN            NaN        NaN   \n",
       "6              NaN           NaN              NaN            NaN        NaN   \n",
       "7              NaN           NaN              NaN            NaN        NaN   \n",
       "8              NaN           NaN              NaN            NaN        NaN   \n",
       "9              NaN           NaN              NaN            NaN        NaN   \n",
       "10             NaN           NaN              NaN            NaN        NaN   \n",
       "11             NaN           NaN              NaN            NaN        NaN   \n",
       "12             NaN           NaN              NaN            NaN        NaN   \n",
       "13             NaN           NaN              NaN            NaN        NaN   \n",
       "14             NaN           NaN              NaN            NaN        NaN   \n",
       "15             NaN           NaN              NaN            NaN        NaN   \n",
       "16             NaN           NaN              NaN            NaN        NaN   \n",
       "17             NaN           NaN              NaN            NaN        NaN   \n",
       "18             NaN           NaN              NaN            NaN        NaN   \n",
       "19             NaN           NaN              NaN            NaN        NaN   \n",
       "20             NaN           NaN              NaN            NaN        NaN   \n",
       "21             NaN           NaN              NaN            NaN        NaN   \n",
       "22             NaN           NaN              NaN            NaN        NaN   \n",
       "23             NaN           NaN              NaN            NaN        NaN   \n",
       "24             NaN           NaN              NaN            NaN        NaN   \n",
       "25             NaN           NaN              NaN            NaN        NaN   \n",
       "26             NaN           NaN              NaN            NaN        NaN   \n",
       "27             NaN           NaN              NaN            NaN        NaN   \n",
       "28             NaN           NaN              NaN            NaN        NaN   \n",
       "29             NaN           NaN              NaN            NaN        NaN   \n",
       "30             NaN           NaN              NaN            NaN        NaN   \n",
       "31             NaN           NaN              NaN            NaN        NaN   \n",
       "32             NaN           NaN              NaN            NaN        NaN   \n",
       "33             NaN           NaN              NaN            NaN        NaN   \n",
       "34             NaN           NaN              NaN            NaN        NaN   \n",
       "35             NaN           NaN              NaN            NaN        NaN   \n",
       "36             NaN           NaN              NaN            NaN        NaN   \n",
       "37             NaN           NaN              NaN            NaN        NaN   \n",
       "38             NaN           NaN              NaN            NaN        NaN   \n",
       "39             NaN           NaN              NaN            NaN        NaN   \n",
       "40             NaN           NaN              NaN            NaN        NaN   \n",
       "41             NaN           NaN              NaN            NaN        NaN   \n",
       "42             NaN           NaN              NaN            NaN        NaN   \n",
       "43             NaN           NaN              NaN            NaN        NaN   \n",
       "44             NaN           NaN              NaN            NaN        NaN   \n",
       "45             NaN           NaN              NaN            NaN        NaN   \n",
       "46             NaN           NaN              NaN            NaN        NaN   \n",
       "47             NaN           NaN              NaN            NaN        NaN   \n",
       "48             NaN           NaN              NaN            NaN        NaN   \n",
       "49             NaN           NaN              NaN            NaN        NaN   \n",
       "50             NaN           NaN              NaN            NaN        NaN   \n",
       "51             NaN           NaN              NaN            NaN        NaN   \n",
       "52             NaN           NaN              NaN            NaN        NaN   \n",
       "53             NaN           NaN              NaN            NaN        NaN   \n",
       "54             NaN           NaN              NaN            NaN        NaN   \n",
       "55             NaN           NaN              NaN            NaN        NaN   \n",
       "56             NaN           NaN              NaN            NaN        NaN   \n",
       "57             NaN           NaN              NaN            NaN        NaN   \n",
       "58             NaN           NaN              NaN            NaN        NaN   \n",
       "59             NaN           NaN              NaN            NaN        NaN   \n",
       "\n",
       "    ... fft_kurt_amp_specright_hand fft_kurt_amp_specleft_wrist  \\\n",
       "0   ...                         NaN                         NaN   \n",
       "1   ...                         NaN                         NaN   \n",
       "2   ...                         NaN                         NaN   \n",
       "3   ...                         NaN                         NaN   \n",
       "4   ...                         NaN                         NaN   \n",
       "5   ...                         NaN                         NaN   \n",
       "6   ...                         NaN                         NaN   \n",
       "7   ...                         NaN                         NaN   \n",
       "8   ...                         NaN                         NaN   \n",
       "9   ...                         NaN                         NaN   \n",
       "10  ...                         NaN                         NaN   \n",
       "11  ...                         NaN                         NaN   \n",
       "12  ...                         NaN                         NaN   \n",
       "13  ...                         NaN                         NaN   \n",
       "14  ...                         NaN                         NaN   \n",
       "15  ...                         NaN                         NaN   \n",
       "16  ...                         NaN                         NaN   \n",
       "17  ...                         NaN                         NaN   \n",
       "18  ...                         NaN                         NaN   \n",
       "19  ...                         NaN                         NaN   \n",
       "20  ...                         NaN                         NaN   \n",
       "21  ...                         NaN                         NaN   \n",
       "22  ...                         NaN                         NaN   \n",
       "23  ...                         NaN                         NaN   \n",
       "24  ...                         NaN                         NaN   \n",
       "25  ...                         NaN                         NaN   \n",
       "26  ...                         NaN                         NaN   \n",
       "27  ...                         NaN                         NaN   \n",
       "28  ...                         NaN                         NaN   \n",
       "29  ...                         NaN                         NaN   \n",
       "30  ...                         NaN                         NaN   \n",
       "31  ...                         NaN                         NaN   \n",
       "32  ...                         NaN                         NaN   \n",
       "33  ...                         NaN                         NaN   \n",
       "34  ...                         NaN                         NaN   \n",
       "35  ...                         NaN                         NaN   \n",
       "36  ...                         NaN                         NaN   \n",
       "37  ...                         NaN                         NaN   \n",
       "38  ...                         NaN                         NaN   \n",
       "39  ...                         NaN                         NaN   \n",
       "40  ...                         NaN                         NaN   \n",
       "41  ...                         NaN                         NaN   \n",
       "42  ...                         NaN                         NaN   \n",
       "43  ...                         NaN                         NaN   \n",
       "44  ...                         NaN                         NaN   \n",
       "45  ...                         NaN                         NaN   \n",
       "46  ...                         NaN                         NaN   \n",
       "47  ...                         NaN                         NaN   \n",
       "48  ...                         NaN                         NaN   \n",
       "49  ...                         NaN                         NaN   \n",
       "50  ...                         NaN                         NaN   \n",
       "51  ...                         NaN                         NaN   \n",
       "52  ...                         NaN                         NaN   \n",
       "53  ...                         NaN                         NaN   \n",
       "54  ...                         NaN                         NaN   \n",
       "55  ...                         NaN                         NaN   \n",
       "56  ...                         NaN                         NaN   \n",
       "57  ...                         NaN                         NaN   \n",
       "58  ...                         NaN                         NaN   \n",
       "59  ...                         NaN                         NaN   \n",
       "\n",
       "   fft_kurt_amp_specright_wrist fft_kurt_amp_specleft_elbow  \\\n",
       "0                           NaN                         NaN   \n",
       "1                           NaN                         NaN   \n",
       "2                           NaN                         NaN   \n",
       "3                           NaN                         NaN   \n",
       "4                           NaN                         NaN   \n",
       "5                           NaN                         NaN   \n",
       "6                           NaN                         NaN   \n",
       "7                           NaN                         NaN   \n",
       "8                           NaN                         NaN   \n",
       "9                           NaN                         NaN   \n",
       "10                          NaN                         NaN   \n",
       "11                          NaN                         NaN   \n",
       "12                          NaN                         NaN   \n",
       "13                          NaN                         NaN   \n",
       "14                          NaN                         NaN   \n",
       "15                          NaN                         NaN   \n",
       "16                          NaN                         NaN   \n",
       "17                          NaN                         NaN   \n",
       "18                          NaN                         NaN   \n",
       "19                          NaN                         NaN   \n",
       "20                          NaN                         NaN   \n",
       "21                          NaN                         NaN   \n",
       "22                          NaN                         NaN   \n",
       "23                          NaN                         NaN   \n",
       "24                          NaN                         NaN   \n",
       "25                          NaN                         NaN   \n",
       "26                          NaN                         NaN   \n",
       "27                          NaN                         NaN   \n",
       "28                          NaN                         NaN   \n",
       "29                          NaN                         NaN   \n",
       "30                          NaN                         NaN   \n",
       "31                          NaN                         NaN   \n",
       "32                          NaN                         NaN   \n",
       "33                          NaN                         NaN   \n",
       "34                          NaN                         NaN   \n",
       "35                          NaN                         NaN   \n",
       "36                          NaN                         NaN   \n",
       "37                          NaN                         NaN   \n",
       "38                          NaN                         NaN   \n",
       "39                          NaN                         NaN   \n",
       "40                          NaN                         NaN   \n",
       "41                          NaN                         NaN   \n",
       "42                          NaN                         NaN   \n",
       "43                          NaN                         NaN   \n",
       "44                          NaN                         NaN   \n",
       "45                          NaN                         NaN   \n",
       "46                          NaN                         NaN   \n",
       "47                          NaN                         NaN   \n",
       "48                          NaN                         NaN   \n",
       "49                          NaN                         NaN   \n",
       "50                          NaN                         NaN   \n",
       "51                          NaN                         NaN   \n",
       "52                          NaN                         NaN   \n",
       "53                          NaN                         NaN   \n",
       "54                          NaN                         NaN   \n",
       "55                          NaN                         NaN   \n",
       "56                          NaN                         NaN   \n",
       "57                          NaN                         NaN   \n",
       "58                          NaN                         NaN   \n",
       "59                          NaN                         NaN   \n",
       "\n",
       "   fft_kurt_amp_specright_elbow fft_kurt_amp_specleft_shoulder  \\\n",
       "0                           NaN                            NaN   \n",
       "1                           NaN                            NaN   \n",
       "2                           NaN                            NaN   \n",
       "3                           NaN                            NaN   \n",
       "4                           NaN                            NaN   \n",
       "5                           NaN                            NaN   \n",
       "6                           NaN                            NaN   \n",
       "7                           NaN                            NaN   \n",
       "8                           NaN                            NaN   \n",
       "9                           NaN                            NaN   \n",
       "10                          NaN                            NaN   \n",
       "11                          NaN                            NaN   \n",
       "12                          NaN                            NaN   \n",
       "13                          NaN                            NaN   \n",
       "14                          NaN                            NaN   \n",
       "15                          NaN                            NaN   \n",
       "16                          NaN                            NaN   \n",
       "17                          NaN                            NaN   \n",
       "18                          NaN                            NaN   \n",
       "19                          NaN                            NaN   \n",
       "20                          NaN                            NaN   \n",
       "21                          NaN                            NaN   \n",
       "22                          NaN                            NaN   \n",
       "23                          NaN                            NaN   \n",
       "24                          NaN                            NaN   \n",
       "25                          NaN                            NaN   \n",
       "26                          NaN                            NaN   \n",
       "27                          NaN                            NaN   \n",
       "28                          NaN                            NaN   \n",
       "29                          NaN                            NaN   \n",
       "30                          NaN                            NaN   \n",
       "31                          NaN                            NaN   \n",
       "32                          NaN                            NaN   \n",
       "33                          NaN                            NaN   \n",
       "34                          NaN                            NaN   \n",
       "35                          NaN                            NaN   \n",
       "36                          NaN                            NaN   \n",
       "37                          NaN                            NaN   \n",
       "38                          NaN                            NaN   \n",
       "39                          NaN                            NaN   \n",
       "40                          NaN                            NaN   \n",
       "41                          NaN                            NaN   \n",
       "42                          NaN                            NaN   \n",
       "43                          NaN                            NaN   \n",
       "44                          NaN                            NaN   \n",
       "45                          NaN                            NaN   \n",
       "46                          NaN                            NaN   \n",
       "47                          NaN                            NaN   \n",
       "48                          NaN                            NaN   \n",
       "49                          NaN                            NaN   \n",
       "50                          NaN                            NaN   \n",
       "51                          NaN                            NaN   \n",
       "52                          NaN                            NaN   \n",
       "53                          NaN                            NaN   \n",
       "54                          NaN                            NaN   \n",
       "55                          NaN                            NaN   \n",
       "56                          NaN                            NaN   \n",
       "57                          NaN                            NaN   \n",
       "58                          NaN                            NaN   \n",
       "59                          NaN                            NaN   \n",
       "\n",
       "   fft_kurt_amp_specshoulder_center fft_kurt_amp_specright_shoulder  \\\n",
       "0                               NaN                             NaN   \n",
       "1                               NaN                             NaN   \n",
       "2                               NaN                             NaN   \n",
       "3                               NaN                             NaN   \n",
       "4                               NaN                             NaN   \n",
       "5                               NaN                             NaN   \n",
       "6                               NaN                             NaN   \n",
       "7                               NaN                             NaN   \n",
       "8                               NaN                             NaN   \n",
       "9                               NaN                             NaN   \n",
       "10                              NaN                             NaN   \n",
       "11                              NaN                             NaN   \n",
       "12                              NaN                             NaN   \n",
       "13                              NaN                             NaN   \n",
       "14                              NaN                             NaN   \n",
       "15                              NaN                             NaN   \n",
       "16                              NaN                             NaN   \n",
       "17                              NaN                             NaN   \n",
       "18                              NaN                             NaN   \n",
       "19                              NaN                             NaN   \n",
       "20                              NaN                             NaN   \n",
       "21                              NaN                             NaN   \n",
       "22                              NaN                             NaN   \n",
       "23                              NaN                             NaN   \n",
       "24                              NaN                             NaN   \n",
       "25                              NaN                             NaN   \n",
       "26                              NaN                             NaN   \n",
       "27                              NaN                             NaN   \n",
       "28                              NaN                             NaN   \n",
       "29                              NaN                             NaN   \n",
       "30                              NaN                             NaN   \n",
       "31                              NaN                             NaN   \n",
       "32                              NaN                             NaN   \n",
       "33                              NaN                             NaN   \n",
       "34                              NaN                             NaN   \n",
       "35                              NaN                             NaN   \n",
       "36                              NaN                             NaN   \n",
       "37                              NaN                             NaN   \n",
       "38                              NaN                             NaN   \n",
       "39                              NaN                             NaN   \n",
       "40                              NaN                             NaN   \n",
       "41                              NaN                             NaN   \n",
       "42                              NaN                             NaN   \n",
       "43                              NaN                             NaN   \n",
       "44                              NaN                             NaN   \n",
       "45                              NaN                             NaN   \n",
       "46                              NaN                             NaN   \n",
       "47                              NaN                             NaN   \n",
       "48                              NaN                             NaN   \n",
       "49                              NaN                             NaN   \n",
       "50                              NaN                             NaN   \n",
       "51                              NaN                             NaN   \n",
       "52                              NaN                             NaN   \n",
       "53                              NaN                             NaN   \n",
       "54                              NaN                             NaN   \n",
       "55                              NaN                             NaN   \n",
       "56                              NaN                             NaN   \n",
       "57                              NaN                             NaN   \n",
       "58                              NaN                             NaN   \n",
       "59                              NaN                             NaN   \n",
       "\n",
       "   fft_kurt_amp_spechead  OoM  \n",
       "0                    NaN  NaN  \n",
       "1                    NaN  NaN  \n",
       "2                    NaN  NaN  \n",
       "3                    NaN  NaN  \n",
       "4                    NaN  NaN  \n",
       "5                    NaN  NaN  \n",
       "6                    NaN  NaN  \n",
       "7                    NaN  NaN  \n",
       "8                    NaN  NaN  \n",
       "9                    NaN  NaN  \n",
       "10                   NaN  NaN  \n",
       "11                   NaN  NaN  \n",
       "12                   NaN  NaN  \n",
       "13                   NaN  NaN  \n",
       "14                   NaN  NaN  \n",
       "15                   NaN  NaN  \n",
       "16                   NaN  NaN  \n",
       "17                   NaN  NaN  \n",
       "18                   NaN  NaN  \n",
       "19                   NaN  NaN  \n",
       "20                   NaN  NaN  \n",
       "21                   NaN  NaN  \n",
       "22                   NaN  NaN  \n",
       "23                   NaN  NaN  \n",
       "24                   NaN  NaN  \n",
       "25                   NaN  NaN  \n",
       "26                   NaN  NaN  \n",
       "27                   NaN  NaN  \n",
       "28                   NaN  NaN  \n",
       "29                   NaN  NaN  \n",
       "30                   NaN  NaN  \n",
       "31                   NaN  NaN  \n",
       "32                   NaN  NaN  \n",
       "33                   NaN  NaN  \n",
       "34                   NaN  NaN  \n",
       "35                   NaN  NaN  \n",
       "36                   NaN  NaN  \n",
       "37                   NaN  NaN  \n",
       "38                   NaN  NaN  \n",
       "39                   NaN  NaN  \n",
       "40                   NaN  NaN  \n",
       "41                   NaN  NaN  \n",
       "42                   NaN  NaN  \n",
       "43                   NaN  NaN  \n",
       "44                   NaN  NaN  \n",
       "45                   NaN  NaN  \n",
       "46                   NaN  NaN  \n",
       "47                   NaN  NaN  \n",
       "48                   NaN  NaN  \n",
       "49                   NaN  NaN  \n",
       "50                   NaN  NaN  \n",
       "51                   NaN  NaN  \n",
       "52                   NaN  NaN  \n",
       "53                   NaN  NaN  \n",
       "54                   NaN  NaN  \n",
       "55                   NaN  NaN  \n",
       "56                   NaN  NaN  \n",
       "57                   NaN  NaN  \n",
       "58                   NaN  NaN  \n",
       "59                   NaN  NaN  \n",
       "\n",
       "[60 rows x 551 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "\n",
    "feature_names = get_feature_names([col[:-2] for col in dataTable.columns[::3]])\n",
    "\n",
    "dataset = pd.DataFrame(index=range(len(annotations)), columns=feature_names+['OoM'])\n",
    "\n",
    "for i in tqdm(range(-len(annotations)),desc='Computing features for the dataset'):\n",
    "    dataTable,OoM = read_data(i)\n",
    "    centerOfMassPosTable = pd.concat([table.apply(lambda row: row.sum()/20,axis=1) for table in table_to_list_xyz_tables(dataTable)],axis=1)\n",
    "    centerOfMassPosTable.columns = ['X','Y','Z']\n",
    "    centerOfMassPosTable = centerOfMassPosTable.apply(lambda row: (row['X'], row['Y'], row['Z']), axis=1)\n",
    "    tableByJoint = group_table_by_joints(dataTable)\n",
    "    jointsDistFromCOM = tableByJoint.sub(centerOfMassPosTable,axis=0)\n",
    "    jointsDistNorm = jointsDistFromCOM.applymap(lambda elem: np.linalg.norm(elem))\n",
    "    jointsDistNormMin, jointsDistNormMax = jointsDistNorm.min(axis=0), jointsDistNorm.max(axis=0)\n",
    "    jointsDistNorm = (jointsDistNorm-jointsDistNormMin)/(jointsDistNormMax-jointsDistNormMin)\n",
    "    featureVector = extractFeatures(np.array(jointsDistNorm))\n",
    "    dataset.loc[i] = np.hstack([featureVector,OoM_to_ord[tuple(map(int, OoM.strip('[]').split('-')))]])\n",
    "#dataset = dataset.astype(dict(zip(list(dataset.columns[:500]),500*[float])).update({dataset.columns[-1]:int}))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(os.path.join(ANNOTATIONS_PATH,\"Dataset.csv\")):\n",
    "    dataset.to_csv(os.path.join(ANNOTATIONS_PATH,\"Dataset.csv\"),index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\[(.*?)\\]'\n",
    "matches = re.findall(pattern, '/n'.join(annotations))\n",
    "\n",
    "\n",
    "results = {}\n",
    "for element in matches:\n",
    "    if ';' not in element:\n",
    "        elem = (int(element.split('-')[0]),int(element.split('-')[1]))\n",
    "        elem = tuple(elem) if elem[0] < elem[1] else tuple(elem[::-1])\n",
    "        if not elem in results.keys():\n",
    "            results[elem] = 1\n",
    "        else:\n",
    "            results[elem] += 1\n",
    "    else:\n",
    "        for twonumbers in element.split(';'):\n",
    "            elem = (int(twonumbers.split('-')[0]),int(twonumbers.split('-')[1]))\n",
    "            elem = tuple(elem) if elem[0] < elem[1] else tuple(elem[::-1])\n",
    "            if not elem in results.keys():\n",
    "                results[elem] = 1\n",
    "            else:\n",
    "                results[elem] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "\n",
    "labels2 = list(results.keys())\n",
    "x_labels = list(sorted(list(set(labels2))))\n",
    "\n",
    "y_values = [results[label] for label in x_labels ]\n",
    "\n",
    "average = sum(y_values) / len(y_values)\n",
    "\n",
    "#for i,value in reversed(list(enumerate(y_values))):\n",
    "#    if value < floor(average):\n",
    "#        x_labels.pop(i)\n",
    "#        y_values.pop(i)\n",
    "\n",
    "#average = sum(y_values)/len(y_values)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.axhline(average, color='blue', linestyle='--', zorder=0)\n",
    "plt.bar(range(0,len(x_labels)), y_values, color='blue', label='Risultati con tutte le annotazioni')\n",
    "plt.xticks(range(0,len(x_labels)),x_labels,rotation=45, ha='right')\n",
    "plt.yticks(range(0,max(y_values)))\n",
    "plt.xlabel('Joints')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.title('Distribuzione joints dataset')\n",
    "\n",
    "plt.annotate(f'Media con annotazioni: {average:.2f}', xy=(6, average), xytext=(2,average+3),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->'), color='black')\n",
    "\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \"To be edge or not to be edge? That is the question\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68ceb99bb094dfa9109794b7dab3c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Full process progression:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d980b62c1243d4bdaae9d600f7be88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing class (12, 14) :   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm \n",
    "import time\n",
    "from collections import deque\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ANNOTATIONS_PATH = 'data'\n",
    "dataset = pd.read_csv(os.path.join(ANNOTATIONS_PATH,'Dataset.csv'))\n",
    "\n",
    "dataset = dataset.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "if not 'y_true' in globals() and not 'y_true' in locals():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "else:\n",
    "    print(\"Model already present\")\n",
    "num_classes = 1#len(dataset['OoM'].value_counts())*( (-1*len(y_true)!=0) + len(y_true)==0)\n",
    "outer_bar = tqdm(total=num_classes,desc=\"Full process progression: \")\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "for class_considered in range(num_classes):\n",
    "    y_class_true = []\n",
    "    y_class_pred = []\n",
    "    target_label = list(dataset['OoM'].value_counts().index)[class_considered]\n",
    "\n",
    "    question = 'Is '+str(ord_to_OoM[int(target_label)])+'?'\n",
    "    dataset[question] = dataset['OoM'] == target_label\n",
    "    labels = np.array(dataset[question].astype(int))\n",
    "\n",
    "    features = np.array(dataset.drop('OoM',axis=1).drop(question,axis=1))\n",
    "\n",
    "\n",
    "    # Create an empty FIFO queue\n",
    "    training_queue = deque()\n",
    "\n",
    "    # Add elements to the queue\n",
    "    for i in range(len(labels)):\n",
    "        training_queue.append(i)\n",
    "\n",
    "    testing_queue = deque()\n",
    "\n",
    "    num_sample_testing = 1\n",
    "    num_epochs = floor(len(labels)/num_sample_testing)\n",
    "    inner_bar = tqdm(total=num_epochs, desc=f\"Processing class {ord_to_OoM[int(target_label)]} \",leave=False)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        mask = np.ones(len(labels),dtype=bool)\n",
    "        for j in range(num_sample_testing):\n",
    "            test_indx = training_queue.popleft()\n",
    "            mask[test_indx] = False\n",
    "            testing_queue.append(test_indx)\n",
    "        train_features = features[mask]\n",
    "        train_labels = labels[mask]\n",
    "\n",
    "        # Train the model on training data\n",
    "        rf.fit(train_features, train_labels)\n",
    "\n",
    "        test_features = features[np.logical_not(mask)]\n",
    "        test_labels = labels[np.logical_not(mask)]\n",
    "        # Use the forest's predict method on the test data\n",
    "        RF_prediction = rf.predict(test_features)\n",
    "        for j in range(num_sample_testing):\n",
    "            if len(RF_prediction.shape) == 1:\n",
    "                RF_prediction = RF_prediction[:,np.newaxis]\n",
    "                test_labels = test_labels[:,np.newaxis]\n",
    "            y_class_true.append(int(test_labels[j,:].all()))\n",
    "            y_class_pred.append(int((RF_prediction[j,:] > 0.5).all()))\n",
    "            training_queue.append(testing_queue.popleft())\n",
    "        time.sleep(0.01)\n",
    "        inner_bar.update(1)\n",
    "    inner_bar.close()\n",
    "    \n",
    "    y_true.append(y_class_true)\n",
    "    y_pred.append(y_class_pred)\n",
    "    time.sleep(0.01)\n",
    "    outer_bar.update(1)\n",
    "    \n",
    "outer_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced class weight & RandomSearch hyperparam tuning & sensitivity as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is (12, 14)?\n",
      "{'max_depth': 6, 'min_samples_split': 3, 'n_estimators': 51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8f97b5e0784a17bc2ee677835dcd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[50  1]\n",
      " [ 8  1]] \n",
      " 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import confusion_matrix, make_scorer, recall_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "target_label = list(dataset['OoM'].value_counts().index)[0]\n",
    "question = 'Is '+str(ord_to_OoM[int(target_label)])+'?'\n",
    "print(question)\n",
    "dataset[question] = dataset['OoM'] == target_label\n",
    "labels = np.array(dataset[question].astype(int))\n",
    "features = np.array(dataset.drop('OoM',axis=1).drop(question,axis=1))\n",
    "n = len(labels)\n",
    "y_class_true = []\n",
    "y_class_pred = []\n",
    "rf = RandomForestClassifier(class_weight='balanced',n_jobs=-1)\n",
    "\n",
    "# Use specificity as the scoring metric\n",
    "def sensitivity_scoring(y_true, y_pred):\n",
    "    conf_matrix = {\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0}\n",
    "    for y_t,y_p in zip(y_true,y_pred):\n",
    "        conf_matrix[(\"T\" if y_t==y_p else \"F\") + (\"P\" if y_t==1 else \"N\")] += 1\n",
    "    TP = conf_matrix['TP']\n",
    "    FN = conf_matrix['FN']\n",
    "    if TP + FN == 0:\n",
    "        return 0\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "\n",
    "# Define a parameter distribution to sample from\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 100),\n",
    "    'max_depth': randint(2, 10),\n",
    "    'min_samples_split': randint(2, 5)\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1, scoring=make_scorer(sensitivity_scoring))\n",
    "\n",
    "# Fit the model with different hyperparameters\n",
    "random_search.fit(features[:-10], labels[:-10])\n",
    "\n",
    "# Get the best hyperparameters and the best model\n",
    "rnd_best_params = random_search.best_params_\n",
    "print(rnd_best_params)\n",
    "\n",
    "\n",
    "# Define a parameter grid to search over\n",
    "#param_grid = {\n",
    "#    'n_estimators': list(range(10,200,10)),\n",
    "#    'max_depth': list(range(2,10))\n",
    "#}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "#grid_search = GridSearchCV( estimator=rf, param_grid=param_grid, cv=5, scoring=make_scorer(specificity_scoring))\n",
    "\n",
    "# Fit the model with different hyperparameters\n",
    "#grid_search.fit(features,labels)#np.delete(features,slice(len(features)-5,len(features)-1),axis=0),np.delete(labels,slice(len(labels)-5,len(labels)-1),axis=0))\n",
    "\n",
    "# Get the best hyperparameters and the best model\n",
    "#best_params = grid_search.best_params_\n",
    "#print(grid_search.best_score_)\n",
    "\n",
    "rf = RandomForestClassifier(**rnd_best_params)\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "#print(best_params)\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels)\n",
    "    \n",
    "    test_features = features[i].reshape(1, -1)\n",
    "    test_labels = labels[i]\n",
    "    rf_prediction = rf.predict(test_features)\n",
    "    \n",
    "    y_class_true.append(test_labels)\n",
    "    y_class_pred.append(rf_prediction[0])\n",
    "\n",
    "# Print confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_class_true, y_class_pred),'\\n',recall_score(y_class_true,y_class_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd541c3bcd04df688102926c227ea67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  0]\n",
      " [ 9  0]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced',n_jobs=-1)\n",
    "n = len(labels)\n",
    "y_class_true = []\n",
    "y_class_pred = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels)\n",
    "    \n",
    "    test_features = features[i].reshape(1, -1)\n",
    "    test_labels = labels[i]\n",
    "    rf_prediction = rf.predict(test_features)\n",
    "    \n",
    "    y_class_true.append(test_labels)\n",
    "    y_class_pred.append(rf_prediction[0])\n",
    "    \n",
    "y_class_true = np.array(y_class_true)\n",
    "y_class_pred = np.array(y_class_pred)\n",
    "print(confusion_matrix(y_class_true,y_class_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac9c161c60e49b9b41b4b2b5ffe6aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  1]\n",
      " [ 8  1]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "ros = RandomOverSampler()\n",
    "n = len(labels)\n",
    "y_class_true = []\n",
    "y_class_pred = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "    train_features_resampled, train_labels_resampled = ros.fit_resample(train_features, train_labels)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features_resampled, train_labels_resampled)\n",
    "    \n",
    "    test_features = features[i].reshape(1, -1)\n",
    "    test_labels = labels[i]\n",
    "    rf_prediction = rf.predict(test_features)\n",
    "    \n",
    "    y_class_true.append(test_labels)\n",
    "    y_class_pred.append(rf_prediction[0])\n",
    "\n",
    "y_class_true = np.array(y_class_true)\n",
    "y_class_pred = np.array(y_class_pred)\n",
    "print(confusion_matrix(y_class_true,y_class_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2719cb16e5f84c3ebcbf41b8f602f458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39 12]\n",
      " [ 3  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "ruc = RandomUnderSampler()\n",
    "n = len(labels)\n",
    "y_class_true = []\n",
    "y_class_pred = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "    train_features_resampled, train_labels_resampled = ruc.fit_resample(train_features, train_labels)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features_resampled, train_labels_resampled)\n",
    "    \n",
    "    test_features = features[i].reshape(1, -1)\n",
    "    test_labels = labels[i]\n",
    "    rf_prediction = rf.predict(test_features)\n",
    "    \n",
    "    y_class_true.append(test_labels)\n",
    "    y_class_pred.append(rf_prediction[0])\n",
    "\n",
    "y_class_true = np.array(y_class_true)\n",
    "y_class_pred = np.array(y_class_pred)\n",
    "print(confusion_matrix(y_class_true,y_class_pred))\n",
    "cm = confusion_matrix(y_class_true,y_class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7155476fb842e088fec895bacb9f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  2]\n",
      " [ 6  3]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create an instance of SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto')\n",
    "\n",
    "n = len(labels)\n",
    "y_class_true = []\n",
    "y_class_pred = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "    train_features_resampled, train_labels_resampled = smote.fit_resample(train_features, train_labels)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features_resampled, train_labels_resampled)\n",
    "    \n",
    "    test_features = features[i].reshape(1, -1)\n",
    "    test_labels = labels[i]\n",
    "    rf_prediction = rf.predict(test_features)\n",
    "    \n",
    "    y_class_true.append(test_labels)\n",
    "    y_class_pred.append(rf_prediction[0])\n",
    "\n",
    "y_class_true = np.array(y_class_true)\n",
    "y_class_pred = np.array(y_class_pred)\n",
    "print(confusion_matrix(y_class_true,y_class_pred))\n",
    "cm = confusion_matrix(y_class_true,y_class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Borderline-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e0e5fa282444e0a3f21f060e26a5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  0]\n",
      " [ 6  3]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "# Create an instance of Borderline-SMOTE\n",
    "borderline_smote = BorderlineSMOTE(sampling_strategy='auto')\n",
    "n = len(labels)\n",
    "y_class_true = []\n",
    "y_class_pred = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "    train_features_resampled, train_labels_resampled = borderline_smote.fit_resample(train_features, train_labels)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features_resampled, train_labels_resampled)\n",
    "    \n",
    "    test_features = features[i].reshape(1, -1)\n",
    "    test_labels = labels[i]\n",
    "    rf_prediction = rf.predict(test_features)\n",
    "    \n",
    "    y_class_true.append(test_labels)\n",
    "    y_class_pred.append(rf_prediction[0])\n",
    "\n",
    "y_class_true = np.array(y_class_true)\n",
    "y_class_pred = np.array(y_class_pred)\n",
    "print(confusion_matrix(y_class_true,y_class_pred))\n",
    "cm = confusion_matrix(y_class_true,y_class_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adasyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149d9f6967634086923ecb94dbffe9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  0]\n",
      " [ 5  4]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "# Create an instance of adasyn\n",
    "adasyn = ADASYN(sampling_strategy='auto')\n",
    "n = len(labels)\n",
    "y_class_true = []\n",
    "y_class_pred = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "    train_features_resampled, train_labels_resampled = adasyn.fit_resample(train_features, train_labels)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features_resampled, train_labels_resampled)\n",
    "    \n",
    "    test_features = features[i].reshape(1, -1)\n",
    "    test_labels = labels[i]\n",
    "    rf_prediction = rf.predict(test_features)\n",
    "    \n",
    "    y_class_true.append(test_labels)\n",
    "    y_class_pred.append(rf_prediction[0])\n",
    "\n",
    "y_class_true = np.array(y_class_true)\n",
    "y_class_pred = np.array(y_class_pred)\n",
    "print(confusion_matrix(y_class_true,y_class_pred))\n",
    "cm = confusion_matrix(y_class_true,y_class_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "- Confusion Matrix\n",
    "\n",
    "| Actual \\ Predicted | True | False |\n",
    "|---------|-----------|------------|\n",
    "| True |   TP      |     FN     |\n",
    "| False |   FP      |     TN     |\n",
    "\n",
    "- Precision:\n",
    " TP / (TP + FP)\n",
    "\n",
    "- Specificity:\n",
    " TN / (TN + FP)\n",
    "\n",
    "- Recall:\n",
    " TP / (TP + FN)\n",
    "\n",
    "- F1 Score:\n",
    " 2 * Precision * Recall / (Precision + Recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21819/1411032177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m  \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_calculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision per class: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21819/1411032177.py\u001b[0m in \u001b[0;36mscore_calculator\u001b[0;34m(y_true, y_pred, overall, normalize)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"TP\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"TN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"FP\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"FN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_p\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverall\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my_p\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"N\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def score_calculator(y_true,y_pred,overall=True,normalize='all'):\n",
    "    '''\n",
    "    Calculates confusion matrix, precision, recall, f1\n",
    "    '''\n",
    "    if not overall:\n",
    "        confusion_matrix = []\n",
    "        for i in range(len(y_true)):\n",
    "            confusion_matrix += [{\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0}]\n",
    "    else:\n",
    "        confusion_matrix = {\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0}\n",
    "    for label in range(len(y_true)):\n",
    "        for y_t,y_p in zip(y_true[label],y_pred[label]):\n",
    "            if not overall:\n",
    "                confusion_matrix[label][(\"T\" if y_t==y_p else \"F\") + (\"P\" if y_t==1 else \"N\")] += 1   \n",
    "            else: \n",
    "               confusion_matrix[(\"T\" if y_t==y_p else \"F\") + (\"P\" if y_t==1 else \"N\")] += 1\n",
    "    if not overall:\n",
    "        temp = []\n",
    "        for label in range(len(y_true)):\n",
    "            temp.append(np.array([[confusion_matrix[label][\"TN\"],confusion_matrix[label][\"FN\"]],[confusion_matrix[label][\"FP\"],confusion_matrix[label][\"TP\"]]], dtype=int if normalize is None else float))\n",
    "        confusion_matrix = np.array(temp)\n",
    "    else:\n",
    "        confusion_matrix = np.array([[confusion_matrix[\"TN\"],confusion_matrix[\"FN\"]],[confusion_matrix[\"FP\"],confusion_matrix[\"TP\"]]], dtype=int if normalize is None else float)\n",
    "    if normalize=='all':\n",
    "        if not overall:\n",
    "            for label in range(len(y_true)):\n",
    "                confusion_matrix[label] /= np.sum(confusion_matrix[label],axis=None)\n",
    "        else:\n",
    "            confusion_matrix /= np.sum(confusion_matrix, axis=None)\n",
    "    if not overall:\n",
    "        precision = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            for label in range(len(y_true)):\n",
    "                precision.append(confusion_matrix[label][0,0]/(confusion_matrix[label][0,0]+confusion_matrix[label][1,0]))\n",
    "                recall.append(confusion_matrix[label][0,0]/(confusion_matrix[label][0,0]+confusion_matrix[label][0,1]))\n",
    "                f1.append(2*precision[-1]*recall[-1]/(precision[-1]+recall[-1]))\n",
    "    else:\n",
    "        precision = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\n",
    "        recall = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[0,1])\n",
    "        f1 = 2*precision*recall / (precision+recall)\n",
    "    return  confusion_matrix,precision,recall,f1\n",
    "\n",
    "cm, precision, recall, f1 = score_calculator(y_true,y_pred,overall=False,normalize=None)\n",
    "print(confusion_matrix(y_true[0],y_pred[0]))\n",
    "print(\"Precision per class: \", precision )\n",
    "print(\"Recall per class: \",recall )\n",
    "print(\"F1 per class: \",f1 )\n",
    "cm = np.hstack(cm[:])\n",
    "plt.figure(figsize=(36, 3))\n",
    "sns.set(font_scale=1.5)  # Adjust the font size for better readability\n",
    "x_labels = [f\"Is Class {ord_to_OoM[int(label)]}\" if i % 2 == 0 else \"Is Not Class\" for i,label in enumerate(list(dataset['OoM'].value_counts().index)*2)]\n",
    "sns.heatmap(cm, annot=True, cmap='Blues',cbar=False, xticklabels=x_labels,yticklabels=[\"|Is Class|\",'|Is Not Class|'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix for all classes')\n",
    "plt.show()\n",
    "\n",
    "cm, precision, recall, f1 = score_calculator(y_true,y_pred)\n",
    "print(\"Precision: \", precision )\n",
    "print(\"Recall: \",recall )\n",
    "print(\"F1: \",f1 )\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)  # Adjust the font size for better readability\n",
    "sns.heatmap(cm, annot=True, cmap='Blues',cbar=False, xticklabels=[\"Is Not class\",\"Is class\"],yticklabels=[\"|Is Class|\",'|Is Not Class|'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix for all classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \"Is it the leg day?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 2 0 4 1 2 3 1 2 1 0 3 3 2 0 0 1 3 2 4 0 2 3 2 0 1 3 4 3 2 3 0 3 1 2\n",
      " 0 0 3 2 4 3 0 2 0 0 4 2 3 2 2 2 3 2 2 4 3 0 1]\n"
     ]
    }
   ],
   "source": [
    "# right leg, left leg, right arm, left arm, head \n",
    "ord_edge_groups = {0: [0,2,3,5], 1:[1,4,6], 2:[7,9,10,12], 3:[8,11,13], 4:[14]}\n",
    "group_names = {0: \"right leg\", 1: \"left leg\", 2:\"right arm\", 3:\"left arm\", 4:\"head\"}\n",
    "rev_ord_edge_groups = {value: key for key, values in ord_edge_groups.items() for value in values}\n",
    "\n",
    "labels = np.array(dataset['OoM'].apply(lambda label: rev_ord_edge_groups[label]))\n",
    "print(labels)\n",
    "y_true = []\n",
    "y_pred = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0.91383818 0.08616182]\n",
      " [0.89642348 0.10357652]\n",
      " [0.89565488 0.10434512]\n",
      " [0.87809684 0.12190316]\n",
      " [0.88036448 0.11963552]\n",
      " [0.91557783 0.08442217]\n",
      " [0.91581105 0.08418895]\n",
      " [0.8432239  0.1567761 ]\n",
      " [0.92079516 0.07920484]\n",
      " [0.91151324 0.08848676]\n",
      " [0.90570939 0.09429061]\n",
      " [0.89819709 0.10180291]\n",
      " [0.87122814 0.12877186]\n",
      " [0.90812537 0.09187463]\n",
      " [0.90663124 0.09336876]] [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create separate SVM classifiers for each label\n",
    "svm_classifiers = { j : SVC(kernel='poly',probability=True, random_state=42) for j in range(train_labels.shape[1]) if any(train_labels[:,j])}\n",
    "[classifier.fit(train_features, train_labels[:,j]) for j,classifier in svm_classifiers.items()]\n",
    "\n",
    "# Predict labels on the test data for each classifier\n",
    "predictions_partial = np.column_stack([svm_classifiers[indx].predict(test_features) for indx in svm_classifiers.keys()])\n",
    "SVM_predictions = np.zeros(test_labels.shape,dtype=int)\n",
    "for i in range(predictions_partial.shape[1]):\n",
    "    SVM_predictions[:,list(svm_classifiers.keys())[i]] = predictions_partial[:,i]\n",
    "print(SVM_predictions)\n",
    "print(svm_classifiers[9].predict_proba(test_features), svm_classifiers[9].predict(test_features))\n",
    "\n",
    "print((SVM_predictions*test_labels).sum()/len(SVM_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0984400488436222\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.set_random_seed(seed=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(train_features.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # You can adjust the number of neurons\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(train_labels.shape[1], activation='sigmoid')  # Sigmoid activation for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for multi-label classification\n",
    "              metrics=['accuracy'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.2, random_state=41)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50  # Number of epochs to avoid overfitting\n",
    "history = model.fit(X_train, y_train, epochs=epochs, verbose=0,validation_data=(X_val,y_val))\n",
    "NN_predictions = model.predict(test_features)\n",
    "\n",
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print((NN_predictions*test_labels).sum()/len(NN_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a multi-label classifier\n",
    "classifier = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=1))\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "KNeighbors_predictions = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "classifier = MultiOutputClassifier(LogisticRegression(penalty=None,random_state=42))\n",
    "\n",
    "#classifier.fit(train_features, train_labels)\n",
    "\n",
    "#LR_predictions = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09716666666666667\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 400, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "RF_predictions = rf.predict(test_features)\n",
    "print((RF_predictions*test_labels).sum()/len(RF_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools needed for visualization\n",
    "#%pip install pydot\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import display\n",
    "\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "\n",
    "# Export the image to a dot file\n",
    "dot_code = export_graphviz(tree, feature_names = dataset.columns[:-1], rounded = True, precision = 1)\n",
    "\n",
    "dot_code = dot_code.replace(']\\n[','],[')\n",
    "\n",
    "import graphviz\n",
    "graph = graphviz.Source(dot_code, format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: mad_right_hand       Importance: 0.05\n",
      "Variable: mean_left_hip        Importance: 0.03\n",
      "Variable: sma_left_hip         Importance: 0.03\n",
      "Variable: mean_right_hip       Importance: 0.02\n",
      "Variable: var_right_foot       Importance: 0.02\n",
      "Variable: var_right_wrist      Importance: 0.02\n",
      "Variable: var_right_elbow      Importance: 0.02\n",
      "Variable: mad_hip_central      Importance: 0.02\n",
      "Variable: mad_right_hip        Importance: 0.02\n",
      "Variable: mad_left_hand        Importance: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(dataset.columns[:-1], importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "for pair in feature_importances[:10]:\n",
    "    if pair[1] >= 0.01:\n",
    "        print('Variable: {:20} Importance: {}'.format(*pair))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)\n",
    "\n",
    "# Extract the two most important features\n",
    "important_indices = [list(dataset.columns[:-1]).index(feature_importances[0][0])]#, list(dataset.columns[:-1]).index(feature_importances[1][0])]\n",
    "train_important = train_features[:, important_indices]\n",
    "test_important = test_features[:, important_indices]\n",
    "\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, train_labels)\n",
    "# Make predictions and determine the error\n",
    "RF_BF_predictions = rf_most_important.predict(test_important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced markers\n",
    "![edges](../resources/reducedMarkersForML.png) ![joints](../resources/reducedMarkersJointsForML.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, 3)</th>\n",
       "      <th>(2, 4)</th>\n",
       "      <th>(3, 5)</th>\n",
       "      <th>(4, 6)</th>\n",
       "      <th>(5, 7)</th>\n",
       "      <th>(6, 8)</th>\n",
       "      <th>(7, 8)</th>\n",
       "      <th>(8, 9)</th>\n",
       "      <th>(9, 10)</th>\n",
       "      <th>(10, 18)</th>\n",
       "      <th>(11, 13)</th>\n",
       "      <th>(12, 14)</th>\n",
       "      <th>(13, 15)</th>\n",
       "      <th>(14, 16)</th>\n",
       "      <th>(15, 17)</th>\n",
       "      <th>(10, 16)</th>\n",
       "      <th>(10, 17)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1, 3)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, 4)</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, 5)</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, 6)</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, 7)</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(6, 8)</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, 8)</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(8, 9)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(9, 10)</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10, 18)</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(11, 13)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(12, 14)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(13, 15)</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(14, 16)</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(15, 17)</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10, 16)</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(10, 17)</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            (1, 3)    (2, 4)    (3, 5)    (4, 6)    (5, 7)    (6, 8)  \\\n",
       "(1, 3)    1.000000  0.125000  0.500000  0.142857  0.333333  0.166667   \n",
       "(2, 4)    0.125000  1.000000  0.142857  0.500000  0.166667  0.333333   \n",
       "(3, 5)    0.500000  0.142857  1.000000  0.166667  0.500000  0.200000   \n",
       "(4, 6)    0.142857  0.500000  0.166667  1.000000  0.200000  0.500000   \n",
       "(5, 7)    0.333333  0.166667  0.500000  0.200000  1.000000  0.250000   \n",
       "(6, 8)    0.166667  0.333333  0.200000  0.500000  0.250000  1.000000   \n",
       "(7, 8)    0.250000  0.200000  0.333333  0.250000  0.500000  0.333333   \n",
       "(8, 9)    0.200000  0.250000  0.250000  0.333333  0.333333  0.500000   \n",
       "(9, 10)   0.166667  0.200000  0.200000  0.250000  0.250000  0.333333   \n",
       "(10, 18)  0.142857  0.166667  0.166667  0.200000  0.200000  0.250000   \n",
       "(11, 13)  0.100000  0.111111  0.111111  0.125000  0.125000  0.142857   \n",
       "(12, 14)  0.111111  0.125000  0.125000  0.142857  0.142857  0.166667   \n",
       "(13, 15)  0.111111  0.125000  0.125000  0.142857  0.142857  0.166667   \n",
       "(14, 16)  0.125000  0.142857  0.142857  0.166667  0.166667  0.200000   \n",
       "(15, 17)  0.125000  0.142857  0.142857  0.166667  0.166667  0.200000   \n",
       "(10, 16)  0.142857  0.166667  0.166667  0.200000  0.200000  0.250000   \n",
       "(10, 17)  0.142857  0.166667  0.166667  0.200000  0.200000  0.250000   \n",
       "\n",
       "            (7, 8)    (8, 9)   (9, 10)  (10, 18)  (11, 13)  (12, 14)  \\\n",
       "(1, 3)    0.250000  0.200000  0.166667  0.142857  0.100000  0.111111   \n",
       "(2, 4)    0.200000  0.250000  0.200000  0.166667  0.111111  0.125000   \n",
       "(3, 5)    0.333333  0.250000  0.200000  0.166667  0.111111  0.125000   \n",
       "(4, 6)    0.250000  0.333333  0.250000  0.200000  0.125000  0.142857   \n",
       "(5, 7)    0.500000  0.333333  0.250000  0.200000  0.125000  0.142857   \n",
       "(6, 8)    0.333333  0.500000  0.333333  0.250000  0.142857  0.166667   \n",
       "(7, 8)    1.000000  0.500000  0.333333  0.250000  0.142857  0.166667   \n",
       "(8, 9)    0.500000  1.000000  0.500000  0.333333  0.166667  0.200000   \n",
       "(9, 10)   0.333333  0.500000  1.000000  0.500000  0.200000  0.250000   \n",
       "(10, 18)  0.250000  0.333333  0.500000  1.000000  0.200000  0.250000   \n",
       "(11, 13)  0.142857  0.166667  0.200000  0.200000  1.000000  0.142857   \n",
       "(12, 14)  0.166667  0.200000  0.250000  0.250000  0.142857  1.000000   \n",
       "(13, 15)  0.166667  0.200000  0.250000  0.250000  0.500000  0.166667   \n",
       "(14, 16)  0.200000  0.250000  0.333333  0.333333  0.166667  0.500000   \n",
       "(15, 17)  0.200000  0.250000  0.333333  0.333333  0.333333  0.200000   \n",
       "(10, 16)  0.250000  0.333333  0.500000  0.500000  0.200000  0.333333   \n",
       "(10, 17)  0.250000  0.333333  0.500000  0.500000  0.250000  0.250000   \n",
       "\n",
       "          (13, 15)  (14, 16)  (15, 17)  (10, 16)  (10, 17)  \n",
       "(1, 3)    0.111111  0.125000  0.125000  0.142857  0.142857  \n",
       "(2, 4)    0.125000  0.142857  0.142857  0.166667  0.166667  \n",
       "(3, 5)    0.125000  0.142857  0.142857  0.166667  0.166667  \n",
       "(4, 6)    0.142857  0.166667  0.166667  0.200000  0.200000  \n",
       "(5, 7)    0.142857  0.166667  0.166667  0.200000  0.200000  \n",
       "(6, 8)    0.166667  0.200000  0.200000  0.250000  0.250000  \n",
       "(7, 8)    0.166667  0.200000  0.200000  0.250000  0.250000  \n",
       "(8, 9)    0.200000  0.250000  0.250000  0.333333  0.333333  \n",
       "(9, 10)   0.250000  0.333333  0.333333  0.500000  0.500000  \n",
       "(10, 18)  0.250000  0.333333  0.333333  0.500000  0.500000  \n",
       "(11, 13)  0.500000  0.166667  0.333333  0.200000  0.250000  \n",
       "(12, 14)  0.166667  0.500000  0.200000  0.333333  0.250000  \n",
       "(13, 15)  1.000000  0.200000  0.500000  0.250000  0.333333  \n",
       "(14, 16)  0.200000  1.000000  0.250000  0.500000  0.333333  \n",
       "(15, 17)  0.500000  0.250000  1.000000  0.333333  0.500000  \n",
       "(10, 16)  0.250000  0.500000  0.333333  1.000000  0.500000  \n",
       "(10, 17)  0.333333  0.333333  0.500000  0.500000  1.000000  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "jointsFromReduced = [1, 3, 5, 7, 2, 4, 6, 8, 9, 9, 9,10,10,16,11,13,15,12,14]\n",
    "jointsToReduced =   [3, 5, 7, 8, 4, 6, 8, 9,10,16,17,16,17,17,13,15,17,14,16]\n",
    "#                 1     2     3     4     5     6     7     8     9      10      11      12      13      14      15       16      17\n",
    "edgesToNodes = [(1,3),(2,4),(3,5),(4,6),(5,7),(6,8),(7,8),(8,9),(9,10),(10,18),(11,13),(12,14),(13,15),(14,16),(15,17),(10,16),(10,17)]\n",
    "allNodes = set(jointsFromReduced).union(set(jointsToReduced))\n",
    "\n",
    "jointsFromReduced = list(map(lambda x: x-1,jointsFromReduced))\n",
    "jointsToReduced = list(map(lambda x: x-1,jointsToReduced))\n",
    "edges = np.array(list(zip(jointsFromReduced,jointsToReduced))+list(zip(jointsToReduced,jointsFromReduced)))\n",
    "\n",
    "adjacencyMatrix = np.zeros((len(allNodes),len(allNodes)),dtype=int)\n",
    "adjacencyMatrix[edges[:,0],edges[:,1]] = 1\n",
    "\n",
    "# Number of nodes in the graph\n",
    "num_nodes = len(adjacencyMatrix)\n",
    "\n",
    "# Function to calculate distances using BFS\n",
    "def calculate_distances(start_node):\n",
    "    distances = np.full(num_nodes, -1)  # Initialize distances to -1 (indicating unreachable)\n",
    "    distances[start_node] = 0  # The distance to itself is 0\n",
    "    queue = deque([start_node])\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        for neighbor in range(num_nodes):\n",
    "            if adjacencyMatrix[node][neighbor] == 1 and distances[neighbor] == -1:\n",
    "                distances[neighbor] = distances[node] + 1\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Calculate distances for every node\n",
    "all_distances = []\n",
    "for node in range(num_nodes):\n",
    "    distances = calculate_distances(node)\n",
    "    all_distances.append(distances)\n",
    "gain = 1/(np.vstack(all_distances)+1)\n",
    "#gain[gain >= 0.6] = 1.\n",
    "#gain[np.logical_and(0.49 <= gain,gain <= 0.51)] = 0.5\n",
    "#gain[gain >= 0.5] = 1.\n",
    "#gain[gain < 0.5] = 0.\n",
    "gain_table = pd.DataFrame(gain, index=[str(value) for value in edgesToNodes], columns=[str(value) for value in edgesToNodes])\n",
    "\n",
    "gain_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (equal distribution): \t\t\t 0.3093028322440087 \n",
      "\n",
      "Random: \t\t\t\t\t 0.30351138146800083 \n",
      "\n",
      "SVM: \t\t\t\t\t\t 0.1934126984126984 \n",
      "\n",
      "Neural Network: \t\t\t\t 0.4522221332194195 \n",
      "\n",
      "Random Forest: \t\t\t\t\t 0.4225855158730159 \n",
      "\n",
      "Random Forest based only on 'mad_right_hand': \t 0.4305111111111111 \n",
      "\n",
      "KNeighbours: \t\t\t\t\t 0.6134920634920635 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_scoring(predictions,test_labels=test_labels,gain=gain):\n",
    "    n_test = len(test_labels)\n",
    "    score = 0.\n",
    "    baseline = 0.\n",
    "    random_scoring = 0.\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    for i in range(n_test):\n",
    "        score += (predictions[i]*gain[:,np.array(test_labels[i]).astype(bool)].max(axis=1)).sum()\n",
    "        baseline += (np.ones(len(predictions[i]))/len(predictions[i])*gain[:,np.array(test_labels[i]).astype(bool)].max(axis=1)).sum()\n",
    "        rand_mat = rng.random(len(predictions[i]))\n",
    "        random_scoring += (rand_mat/rand_mat.sum()*gain[:,np.array(test_labels[i]).astype(bool)].max(axis=1)).sum()\n",
    "    return baseline/n_test, random_scoring/n_test, score/n_test\n",
    "print(\"Baseline (equal distribution): \\t\\t\\t\",calculate_scoring(RF_predictions)[0],'\\n')\n",
    "print(\"Random: \\t\\t\\t\\t\\t\",calculate_scoring(RF_predictions)[1],'\\n')\n",
    "print(\"SVM: \\t\\t\\t\\t\\t\\t\",calculate_scoring(SVM_predictions)[2],'\\n')\n",
    "print(\"Neural Network: \\t\\t\\t\\t\",calculate_scoring(NN_predictions)[2],'\\n')\n",
    "print(\"Random Forest: \\t\\t\\t\\t\\t\",calculate_scoring(RF_predictions)[2],'\\n')\n",
    "print(f\"Random Forest based only on '{feature_importances[0][0]}': \\t\",calculate_scoring(RF_BF_predictions)[2],'\\n')\n",
    "print(\"KNeighbours: \\t\\t\\t\\t\\t\",calculate_scoring(KNeighbors_predictions)[2],'\\n')\n",
    "#print(\"LogisticRession: \\t\\t\\t\\t\",calculate_scoring(LR_predictions)[2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (equal distribution): \t\t\t 0.07058823529411763 \n",
      "\n",
      "Random: \t\t\t\t\t 0.06292047063071406 \n",
      "\n",
      "SVM: \t\t\t\t\t\t 0.06666666666666667 \n",
      "\n",
      "Neural Network: \t\t\t\t 0.0984400488436222 \n",
      "\n",
      "Random Forest: \t\t\t\t\t 0.09716666666666667 \n",
      "\n",
      "Random Forest based only on 'mad_right_hand': \t 0.1016 \n",
      "\n",
      "KNeighbours: \t\t\t\t\t 0.2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def accuracy_on_point_joint(predictions):\n",
    "    if predictions is None:\n",
    "        rng = np.random.default_rng(seed=42)\n",
    "        rand_mat = rng.random(test_labels.shape)\n",
    "        rand_mat = rand_mat/np.sum(rand_mat,axis=1)[:,np.newaxis]\n",
    "        return (rand_mat*test_labels).sum()/len(test_labels)\n",
    "    return (predictions*test_labels).sum()/len(test_labels)\n",
    "\n",
    "print(\"Baseline (equal distribution): \\t\\t\\t\",accuracy_on_point_joint(np.ones(test_labels.shape)/test_labels.shape[1]),'\\n')\n",
    "print(\"Random: \\t\\t\\t\\t\\t\",accuracy_on_point_joint(None),'\\n')\n",
    "print(\"SVM: \\t\\t\\t\\t\\t\\t\",accuracy_on_point_joint(SVM_predictions),'\\n')\n",
    "print(\"Neural Network: \\t\\t\\t\\t\",accuracy_on_point_joint(NN_predictions),'\\n')\n",
    "print(\"Random Forest: \\t\\t\\t\\t\\t\",accuracy_on_point_joint(RF_predictions),'\\n')\n",
    "print(f\"Random Forest based only on '{feature_importances[0][0]}': \\t\",accuracy_on_point_joint(RF_BF_predictions),'\\n')\n",
    "print(\"KNeighbours: \\t\\t\\t\\t\\t\",accuracy_on_point_joint(KNeighbors_predictions),'\\n')\n",
    "#print(\"LogisticRession: \\t\\t\\t\\t\",accuracy_on_point_joint(LR_predictions),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "KNeighbors_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m hamming_loss\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(hamming_loss(predictions,test_labels\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m)\u001b[39m+\u001b[39;49m\u001b[39m0.01\u001b[39;49m))\n\u001b[1;32m      3\u001b[0m \u001b[39m# Suppress the divide by zero warning temporarily\u001b[39;00m\n\u001b[1;32m      4\u001b[0m np\u001b[39m.\u001b[39mseterr(divide\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m, invalid\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2728\u001b[0m, in \u001b[0;36mhamming_loss\u001b[0;34m(y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m   2644\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m   2645\u001b[0m     {\n\u001b[1;32m   2646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2651\u001b[0m )\n\u001b[1;32m   2652\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhamming_loss\u001b[39m(y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2653\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the average Hamming loss.\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m \n\u001b[1;32m   2655\u001b[0m \u001b[39m    The Hamming loss is the fraction of labels that are incorrectly predicted.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2725\u001b[0m \u001b[39m    0.75\u001b[39;00m\n\u001b[1;32m   2726\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2728\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   2729\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m   2731\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/sklearn/metrics/_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    107\u001b[0m     y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: continuous-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "# Suppress the divide by zero warning temporarily\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# Calculate MAPE\n",
    "def calculate_mape(actual, predicted):\n",
    "    epsilon = 1e-10  # Small epsilon value to avoid division by zero\n",
    "    abs_percentage_error = np.abs((actual - predicted) / (actual + epsilon))\n",
    "    mape = np.mean(abs_percentage_error) * 100\n",
    "    return mape\n",
    "\n",
    "# Calculate MAPE, handling division by zero\n",
    "mape = calculate_mape(test_labels, predictions)\n",
    "\n",
    "# Print the MAPE\n",
    "print(\"MAPE:\", mape)\n",
    "\n",
    "errors = abs(predictions - test_labels)# Print out the mean absolute error (mae)\n",
    "#print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "#print(test_labels)\n",
    "print(1-np.mean((errors)/(test_labels)))\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = (errors / test_labels)# Calculate and display accuracy\n",
    "# Check for and handle division by zero\n",
    "print(mape)\n",
    "mape[np.isinf(mape)] = 1.  # Replace inf values with 0\n",
    "#mape[np.isnan(mape)] = 0.\n",
    "\n",
    "accuracy = 1 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy*100, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def group_table_by_joints(table:pd.DataFrame):\n",
    "    columns = [col.replace('_X','') for col in list(table.columns)[::3]]\n",
    "    result = pd.DataFrame(columns=columns)\n",
    "    for j in range(0,table.shape[1],3):\n",
    "        lst = []\n",
    "        for i in range(table.shape[0]):\n",
    "            lst.append(table.iloc[i,j:j+3].values)\n",
    "        result[columns[int(j/3)]] = lst\n",
    "    return result\n",
    "\n",
    "def compute_derivatives(posTable:pd.DataFrame,timeCol,which:set={'speed'}):\n",
    "    out = []\n",
    "    if 'speed' in which:\n",
    "        velocityTable = pd.DataFrame(np.gradient(posTable,timeCol,axis=0),columns=[f'{c}' for c in posTable.columns])\n",
    "        out.append(velocityTable)\n",
    "    if 'acceleration' in which:\n",
    "        accelerationTable = pd.DataFrame(np.gradient(velocityTable,timeCol,axis=0),columns=[f'{c}' for c in velocityTable.columns])\n",
    "        out.append(accelerationTable)\n",
    "    if len(out) == 1:\n",
    "        return out[0]\n",
    "    return out\n",
    "\n",
    "def compute_angular_momentum(posTables,timeCol):\n",
    "    m_lfoot = 0.0145;m_rfoot = 0.0145;m_lank = 0.0465;m_rank = 0.0465\n",
    "    m_lknee = 0.1;m_rknee = 0.1;m_lhip = 0.1;m_hipc = 0.139;m_rhip = 0.1\n",
    "    m_spine = 0.216;m_lhand = 0.006;m_rhand = 0.006;m_lwrist = 0.016;m_rwrist = 0.016\n",
    "    m_lelb = 0.028;m_relb = 0.028;m_lsho = 0.0158;m_shoc = 0.081;m_rsho = 0.0158;m_head = 0.081\n",
    "\n",
    "    mass = [m_lfoot, m_rfoot, m_lank, m_rank, m_lknee,  m_rknee, m_lhip, m_hipc, m_rhip, m_spine, m_lhand, m_rhand, m_lwrist, m_rwrist, m_lelb, m_relb, m_lsho, m_shoc, m_rsho, m_head]\n",
    "    \n",
    "    posTableX, posTableY, posTableZ = posTables\n",
    "    posTableX:pd.DataFrame;posTableY:pd.DataFrame;posTableZ:pd.DataFrame\n",
    "\n",
    "    centerOfMassX = posTableX.dot(mass).div(sum(mass))\n",
    "    centerOfMassY = posTableY.dot(mass).div(sum(mass))\n",
    "    centerOfMassZ = posTableZ.dot(mass).div(sum(mass))\n",
    "\n",
    "    radiusQX = posTableX.sub(centerOfMassX,axis=0)\n",
    "    radiusQY = posTableY.sub(centerOfMassY,axis=0)\n",
    "    radiusQZ = posTableZ.sub(centerOfMassZ,axis=0)\n",
    "\n",
    "    def merge_tables(xTable,yTable,zTable):\n",
    "        mergedTable = pd.DataFrame()\n",
    "        for j in range(xTable.shape[1]):\n",
    "            mergedTable = pd.concat([mergedTable,xTable.iloc[:,j],yTable.iloc[:,j],zTable.iloc[:,j]],axis=1)\n",
    "        return mergedTable\n",
    "    \n",
    "    radiusQTable = merge_tables(radiusQX,radiusQY,radiusQZ)\n",
    "    p = compute_derivatives(merge_tables(posTableX,posTableY,posTableZ),timeCol).mul(np.repeat(mass,3))\n",
    "\n",
    "    def split_table(table,into=\"xyz\"):\n",
    "        if into == \"xyz\":\n",
    "            return table.iloc[:,::3],table.iloc[:,1::3],table.iloc[:,2::3]\n",
    "        elif into == \"points\":\n",
    "            return [table.iloc[:,j:j+3] for j in range(0,table.shape[1],3)]\n",
    "        \n",
    "    p = [point.to_numpy() for point in split_table(p,into=\"points\")]\n",
    "    r = [point.to_numpy() for point in split_table(radiusQTable, into=\"points\")]\n",
    "    l = np.cross(p,r)\n",
    "\n",
    "    angularMomentumTable = radiusQTable*0\n",
    "    for j,point in enumerate(l):\n",
    "        for i in range(len(point)):\n",
    "            angularMomentumTable.iloc[i,3*j:3*j+3] = point[i]\n",
    "    return angularMomentumTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def compute_cosine_similarity_on_joints(featureTable,normalized:None or int=None):\n",
    "    jointsFrom = [1, 3, 5, 7, 2, 4, 6, 8, 8,10,11,13,15,17,12,14,16,18,18]\n",
    "    jointsTo =   [3, 5, 7, 8, 4, 6, 9, 9,10,18,13,15,17,18,14,16,19,19,20]\n",
    "\n",
    "    jointsFrom = list(map(lambda x: x-1,jointsFrom))\n",
    "    jointsTo = list(map(lambda x: x-1,jointsTo))\n",
    "\n",
    "    cosineSim = pd.DataFrame()\n",
    "    columnNames = list(featureTable.columns)\n",
    "    for jointFrom,jointTo in zip(jointsFrom,jointsTo):\n",
    "        cosineSim[columnNames[jointFrom]+\" - \"+columnNames[jointTo]] = featureTable.apply(lambda row: 2 - cosine(row[columnNames[jointFrom]],row[columnNames[jointTo]]),axis=1)\n",
    "    return cosineSim if normalized is None else cosineSim.div(cosineSim.max(axis=1))*normalized\n",
    "\n",
    "from numpy.linalg import norm\n",
    "def compute_inverse_difference_on_joints(featureTable):\n",
    "    jointsFrom = [1, 3, 5, 7, 2, 4, 6, 9, 8,10,11,13,15,17,12,14,16,19,18]\n",
    "    jointsTo =   [3, 5, 7, 8, 4, 6, 9, 8,10,18,13,15,17,18,14,16,19,18,20]\n",
    "    jointsFrom = list(map(lambda x: x-1,jointsFrom))\n",
    "    jointsTo = list(map(lambda x: x-1,jointsTo))\n",
    "\n",
    "    invDiffModulus = pd.DataFrame()\n",
    "    eps = 10**(-9)  # To avoid division by 0\n",
    "    columnNames = list(featureTable.columns)\n",
    "    for jointFrom,jointTo in zip(jointsFrom,jointsTo):\n",
    "        invDiffModulus[columnNames[jointFrom]+\" - \"+columnNames[jointTo]] = featureTable.apply(lambda row: norm(row[columnNames[jointFrom]]-row[columnNames[jointTo]])+eps,axis=1)\n",
    "    return 1 / invDiffModulus\n",
    "    \n",
    "\n",
    "def group_table_by_joints(table):\n",
    "    columns = [col.replace('_X','') for col in list(table.columns)[::3]]\n",
    "    result = pd.DataFrame(columns=columns)\n",
    "    for j in range(0,table.shape[1],3):\n",
    "        lst = []\n",
    "        for i in range(table.shape[0]):\n",
    "            lst.append(table.iloc[i,j:j+3].values)\n",
    "        result[columns[int(j/3)]] = lst\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional functions for rotating, filtering, plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_rotate_table(picked:int,posTable:pd.DataFrame):\n",
    "    if 1 <= picked <= 10:\n",
    "        posTableY = posTable.iloc[:,::3]\n",
    "        posTableZ = posTable.iloc[:,1::3]\n",
    "        posTableX = posTable.iloc[:,2::3]\n",
    "    elif 11 <= picked <= 16:\n",
    "        posTableX = posTable.iloc[:,::3]\n",
    "        posTableZ = posTable.iloc[:,1::3]\n",
    "        posTableY = posTable.iloc[:,2::3]\n",
    "    elif (17 <= picked <= 20 and picked != 19) or picked >= 25:\n",
    "        posTableY = posTable.iloc[:,::3]\n",
    "        posTableX = posTable.iloc[:,1::3]\n",
    "        posTableZ = posTable.iloc[:,2::3]\n",
    "    else:\n",
    "        posTableX = posTable.iloc[:,::3]\n",
    "        posTableY = posTable.iloc[:,1::3]\n",
    "        posTableZ = posTable.iloc[:,2::3]\n",
    "\n",
    "    posTableX = posTableX.rename(columns=dict(zip(list(posTableX),[name.replace('Z','X').replace('Y','X') for name in list(posTableX)])))\n",
    "    posTableY = posTableY.rename(columns=dict(zip(list(posTableY),[name.replace('Z','Y').replace('X','Y') for name in list(posTableY)])))\n",
    "    posTableZ = posTableZ.rename(columns=dict(zip(list(posTableZ),[name.replace('X','Z').replace('Y','Z') for name in list(posTableZ)])))\n",
    "\n",
    "    return (posTableX,posTableY,posTableZ)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.widgets import Slider\n",
    "import numpy as np\n",
    "\n",
    "def draw_skeleton(posTable:tuple,picked:int):\n",
    "    posTableX,posTableY,posTableZ = posTable\n",
    "    posTableX:pd.DataFrame\n",
    "    posTableY:pd.DataFrame\n",
    "    posTableZ:pd.DataFrame\n",
    "    plt.close(\"all\")\n",
    "    %matplotlib\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "    minMax = np.zeros((2,3))\n",
    "    minMax[0,:] = [posTableX.values.min(),posTableY.values.min(),posTableZ.values.min()]\n",
    "    minMax[1,:] = [posTableX.values.max(),posTableY.values.max(),posTableZ.values.max()]\n",
    "\n",
    "    print(minMax)\n",
    "\n",
    "    # Set appropriate axis limits\n",
    "    ax.set_xlim([minMax[0,0],minMax[1,0]])\n",
    "    ax.set_ylim([minMax[0,1],minMax[1,1]])\n",
    "    ax.set_zlim([minMax[0,2],minMax[1,2]])\n",
    "\n",
    "    # Set the window title\n",
    "    fig.canvas.manager.window.title(\"3D Movement\\t(Scroll with mouse wheel)\")\n",
    "\n",
    "    # Set the initial time index\n",
    "    time_index = 0\n",
    "\n",
    "    # Function to update the plot based on the slider value\n",
    "    def update_plot(val):\n",
    "        ax.cla()  # Clear the previous plot\n",
    "\n",
    "        # Filter the data based on the current time index\n",
    "        filteredX = posTableX.iloc[val]\n",
    "        filteredY = posTableY.iloc[val]\n",
    "        filteredZ = posTableZ.iloc[val]\n",
    "\n",
    "        ax.scatter(filteredX,filteredY,filteredZ)\n",
    "\n",
    "        ax.set_xlim([minMax[0,0],minMax[1,0]])\n",
    "        ax.set_ylim([minMax[0,1],minMax[1,1]])\n",
    "        ax.set_zlim([minMax[0,2],minMax[1,2]])\n",
    "\n",
    "        ax.set_xlabel('X', fontsize=12)\n",
    "        ax.set_ylabel('Y', fontsize=12)\n",
    "        ax.set_zlabel('Z', fontsize=12)\n",
    "        ax.set_title(\"Movement \"+str(picked))\n",
    "\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    # Create a slider widget\n",
    "    slider_ax = plt.axes([0.2, 0.03, 0.6, 0.03])\n",
    "    maxValue = posTable.shape[0]-1\n",
    "    slider = Slider(slider_ax, 'TimeIndex:', 0, maxValue, valinit=time_index, valstep=1)\n",
    "\n",
    "\n",
    "    # Define a function to update the slider value with the mouse wheel\n",
    "    def on_scroll(event):\n",
    "        if event.button == 'down':\n",
    "            if slider.val + slider.valstep*5 <= maxValue:\n",
    "                slider.set_val(slider.val + slider.valstep*2)\n",
    "        elif event.button == 'up':\n",
    "            if slider.val - slider.valstep*10 >= 0:\n",
    "                slider.set_val(slider.val - slider.valstep*2)\n",
    "\n",
    "\n",
    "    # Connect the mouse wheel event to the function\n",
    "    fig.canvas.mpl_connect('scroll_event', on_scroll)\n",
    "\n",
    "\n",
    "    # Register the update_plot function with the slider widget\n",
    "    slider.on_changed(update_plot)\n",
    "\n",
    "    # Initial plot\n",
    "    update_plot(time_index)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rknee-Rhip\n",
      "Rhip-Chip\n",
      "Chip-spine\n",
      "spine-Cshoulder\n",
      "Rshoulder-Cshoulder\n",
      "(36, 16)\n"
     ]
    }
   ],
   "source": [
    "edgeNames = ['Lfoot-Lankle',           \n",
    "          'Rfoot-Rankle',           \n",
    "          'Lankle-Lknee',           \n",
    "          'Rankle-Rknee',           \n",
    "          'Lknee-Lhip',\n",
    "          'Rknee-Rhip',\n",
    "          'Lhip-Chip',\n",
    "          'Rhip-Chip',\n",
    "          'Chip-spine',\n",
    "          'spine-Cshoulder',\n",
    "          'Lhand-Lwrist',\n",
    "          'Rhand-Rwrist',\n",
    "          'Lwrist-Lelbow',\n",
    "          'Rwrist-Relbow',\n",
    "          'Lelbow-Lshoulder',\n",
    "          'Relbow-Rshoulder',       \n",
    "          'Lshoulder-Cshoulder',    \n",
    "          'Rshoulder-Cshoulder',    \n",
    "          'Cshoulder-head']         \n",
    "jointNames = [  'left_foot',\n",
    "                'right_foot',\n",
    "                'left_ankle',\n",
    "                'right_ankle',\n",
    "                'left_knee',\n",
    "                'right_knee',\n",
    "                'left_hip',\n",
    "                'hip_center',\n",
    "                'right_hip', \n",
    "                'spine',\n",
    "                'left_hand',\n",
    "                'right_hand',\n",
    "                'left_wrist',\n",
    "                'right_wrist',\n",
    "                'left_elbow',\n",
    "                'right_elbow',\n",
    "                'left_shoulder',\n",
    "                'shoulder_center',\n",
    "                'right_shoulder',\n",
    "                'head'  ]\n",
    "y_mask =     {1:    [edgeNames.index('Rshoulder-Cshoulder')],\n",
    "              2:    [edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder'),edgeNames.index('Cshoulder-head')],\n",
    "              3:    [edgeNames.index('Rhip-Chip')],\n",
    "              4:    [edgeNames.index('Lshoulder-Cshoulder')],\n",
    "              5:    [edgeNames.index('Cshoulder-head')],\n",
    "              6:    [edgeNames.index('Rankle-Rknee')],\n",
    "              7:    [edgeNames.index('Lshoulder-Cshoulder')],\n",
    "              8:    [edgeNames.index('Rshoulder-Cshoulder')],\n",
    "              9:    [edgeNames.index('Rankle-Rknee'),edgeNames.index('Rknee-Rhip'),edgeNames.index('Rhip-Chip')],\n",
    "              10:   [edgeNames.index('Lshoulder-Cshoulder')],\n",
    "              11:   [edgeNames.index('Rhand-Rwrist')],\n",
    "              12:   [edgeNames.index('Relbow-Rshoulder'),edgeNames.index('Rshoulder-Cshoulder'),edgeNames.index('Cshoulder-head')],\n",
    "              13:   [edgeNames.index('Lankle-Lknee'),edgeNames.index('Lknee-Lhip')],\n",
    "              14:   [edgeNames.index('Lknee-Lhip')],\n",
    "              15:   [edgeNames.index('Lwrist-Lelbow'),edgeNames.index('Lelbow-Lshoulder')],\n",
    "              16:   [edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder')],\n",
    "              17:   [edgeNames.index('Rknee-Rhip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Lhip-Chip'),edgeNames.index('Lknee-Lhip')],\n",
    "              18:   [],\n",
    "              19:   [],\n",
    "              20:   [edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder')],\n",
    "              21:   [edgeNames.index('Relbow-Rshoulder')],\n",
    "              22:   [edgeNames.index('Lelbow-Lshoulder'),edgeNames.index('Lshoulder-Cshoulder')],\n",
    "              23:   [edgeNames.index('Lelbow-Lshoulder'),edgeNames.index('Lshoulder-Cshoulder')],\n",
    "              24:   [edgeNames.index('Rwrist-Relbow')],\n",
    "              25:   [edgeNames.index('Lknee-Lhip'),edgeNames.index('Lhip-Chip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Rknee-Rhip')],\n",
    "              26:   [edgeNames.index('Lknee-Lhip'),edgeNames.index('Lhip-Chip'),edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder')],\n",
    "              27:   [edgeNames.index('Lshoulder-Cshoulder'),edgeNames.index('Lelbow-Lshoulder')],\n",
    "              28:   [edgeNames.index('Lknee-Lhip'),edgeNames.index('Lhip-Chip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Rknee-Rhip')],\n",
    "              29:   [edgeNames.index('Lknee-Lhip'),edgeNames.index('Lhip-Chip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Rknee-Rhip')],\n",
    "              30:   [edgeNames.index('Rknee-Rhip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder')],\n",
    "              31:   [edgeNames.index('Rhip-Chip'),edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder')],\n",
    "              32:   [edgeNames.index('Rknee-Rhip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder'),edgeNames.index('Rshoulder-Cshoulder'),edgeNames.index('Relbow-Rshoulder')],\n",
    "              33:   [edgeNames.index('Lelbow-Lshoulder'),edgeNames.index('Lshoulder-Cshoulder'),edgeNames.index('Rshoulder-Cshoulder'),edgeNames.index('Relbow-Rshoulder')],\n",
    "              34:   [edgeNames.index('Lknee-Lhip'),edgeNames.index('Lhip-Chip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Rknee-Rhip')],\n",
    "              35:   [edgeNames.index('Rknee-Rhip'),edgeNames.index('Rhip-Chip'),edgeNames.index('Chip-spine'),edgeNames.index('spine-Cshoulder'),edgeNames.index('Rshoulder-Cshoulder')],\n",
    "              36:   [edgeNames.index('Relbow-Rshoulder')]\n",
    "              }\n",
    "for edgeIndex in y_mask[35]:\n",
    "    print(edgeNames[edgeIndex])\n",
    "\n",
    "y_trans=[]\n",
    "for key, value in y_mask.items():\n",
    "    y_trans.append(value)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer \n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y_converted = mlb.fit_transform(y_trans)\n",
    "\n",
    "mlb.classes_   \n",
    "\n",
    "print(y_converted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m mlb \u001b[39m=\u001b[39m MultiLabelBinarizer()\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m items \u001b[39min\u001b[39;00m y_mask:\n\u001b[0;32m---> 19\u001b[0m     y_converted \u001b[39m=\u001b[39m mlb\u001b[39m.\u001b[39;49mfit_transform(items)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(y_converted)\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m i,indx_in_drive \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(DRIVE_POS_MAP\u001b[39m.\u001b[39mkeys()):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:819\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    817\u001b[0m class_mapping \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[1;32m    818\u001b[0m class_mapping\u001b[39m.\u001b[39mdefault_factory \u001b[39m=\u001b[39m class_mapping\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m\n\u001b[0;32m--> 819\u001b[0m yt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(y, class_mapping)\n\u001b[1;32m    821\u001b[0m \u001b[39m# sort classes and reorder columns\u001b[39;00m\n\u001b[1;32m    822\u001b[0m tmp \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(class_mapping, key\u001b[39m=\u001b[39mclass_mapping\u001b[39m.\u001b[39mget)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:890\u001b[0m, in \u001b[0;36mMultiLabelBinarizer._transform\u001b[0;34m(self, y, class_mapping)\u001b[0m\n\u001b[1;32m    888\u001b[0m indptr \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39marray(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m0\u001b[39m])\n\u001b[1;32m    889\u001b[0m unknown \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m--> 890\u001b[0m \u001b[39mfor\u001b[39;00m labels \u001b[39min\u001b[39;00m y:\n\u001b[1;32m    891\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    892\u001b[0m     \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "columns = [elem for tup in [('X_'+pair[0]+'_avg','X_'+pair[1]+'_var') for pair in zip(jointNames,jointNames)] for elem in tup] + ['Y_OneHot_'+edgeName for edgeName in edgeNames]\n",
    "column_types = [float if j < len(columns)-len(edgeNames) else bool for j,k in enumerate(columns)]\n",
    "dataset = pd.DataFrame(index=np.arange(len(DRIVE_POS_MAP.keys())), columns=columns).astype(dict(zip(columns,column_types)))\n",
    "\n",
    "# Helper function to calculate mean and variance of every joint\n",
    "calc_mean_and_var = lambda j: (\n",
    "    speedTable.iloc[:, j : j + 3].apply(lambda row: norm(row), axis=1).mean(),\n",
    "    speedTable.iloc[:, j : j + 3].apply(lambda row: norm(row), axis=1).var(),\n",
    ")\n",
    "oneHotEncoding = np.zeros(len(edgeNames),dtype=bool)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "for items in y_mask:\n",
    "    y_converted = mlb.fit_transform(items)\n",
    "    print(y_converted)\n",
    "\n",
    "for i,indx_in_drive in enumerate(DRIVE_POS_MAP.keys()):\n",
    "    data = DRIVE_POS_MAP[indx_in_drive]\n",
    "    table = pd.read_table(data[0])\n",
    "    startEndSeconds = data[1]\n",
    "    timeCol = table[table[\"Time\"].between(startEndSeconds[0],startEndSeconds[1])][\"Time\"]\n",
    "    table = table[table[\"Time\"].between(startEndSeconds[0],startEndSeconds[1])].iloc[:,1:]\n",
    "    speedTable = compute_derivatives(table,timeCol) \n",
    "    accelerationTable = compute_derivatives(speedTable, timeCol)\n",
    "\n",
    "    # Assign the calculated values to the dataset\n",
    "    for j in range(0,len(speedTable.columns),3):\n",
    "        dataset.loc[i, f'X_{jointNames[j//3]}_avg'], dataset.loc[i, f'X_{jointNames[j//3]}_var'] = calc_mean_and_var(j)\n",
    "\n",
    "    # Assign the calculated values to the dataset\n",
    "    #for j in range(0,len(accelerationTable.columns),3):\n",
    "    #    dataset.loc[i, f'X_{jointNames[j//3]}_avg'], dataset.loc[i, f'X_{jointNames[j//3]}_var'] = calc_mean_and_var(j)\n",
    "\n",
    "    \n",
    "\n",
    "    oneHotEncoding[y_mask[indx_in_drive]] = 1\n",
    "    dataset.iloc[i,-len(edgeNames):] = oneHotEncoding\n",
    "    oneHotEncoding[y_mask[indx_in_drive]] = 0\n",
    "\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 40)\n"
     ]
    }
   ],
   "source": [
    "X = dataset.iloc[:,:40]\n",
    "y = dataset.iloc[:,40:]\n",
    "df = pd.DataFrame(X)\n",
    "X = df.values\n",
    "df = pd.DataFrame(y)\n",
    "y = df.values\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Hamming Loss: 0.18421052631578946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "\n",
    "# Generate synthetic multi-label dataset\n",
    "#X, y = make_multilabel_classification(n_samples=100, n_features=10, n_classes=5, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create a multi-label classifier\n",
    "classifier = MultiOutputClassifier(KNeighborsClassifier())\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and Hamming loss\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "hamming_loss = hamming_loss(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Hamming Loss:\", hamming_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape,learning_rate):\n",
    "        self.weights = np.random.randn(input_shape[1],input_shape[0])\n",
    "        self.bias = np.random.randn(input_shape[0])\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _sigmoid_deriv(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "        return prediction\n",
    "\n",
    "    def _compute_gradients(self, input_vector, target):\n",
    "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
    "        print(layer_1.shape)\n",
    "        layer_2 = self._sigmoid(layer_1)\n",
    "        prediction = layer_2\n",
    "\n",
    "        derror_dprediction = 2 * (prediction - target)\n",
    "        dprediction_dlayer1 = self._sigmoid_deriv(layer_1)\n",
    "        dlayer1_dbias = 1\n",
    "        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)\n",
    "\n",
    "        derror_dbias = (\n",
    "            derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
    "        )\n",
    "        derror_dweights = (\n",
    "            derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
    "        )\n",
    "\n",
    "        return derror_dbias, derror_dweights\n",
    "\n",
    "    def _update_parameters(self, derror_dbias, derror_dweights):\n",
    "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
    "        self.weights = self.weights - (\n",
    "            derror_dweights * self.learning_rate\n",
    "        )\n",
    "\n",
    "    def train(self, input_vectors, targets, iterations):\n",
    "\n",
    "        cumulative_errors = []\n",
    "\n",
    "        for current_iteration in range(iterations):\n",
    "\n",
    "            # Pick a data instance at random\n",
    "\n",
    "            random_data_index = np.random.randint(len(input_vectors))\n",
    "\n",
    "\n",
    "            input_vector = input_vectors[random_data_index]\n",
    "\n",
    "            target = targets[random_data_index]\n",
    "\n",
    "\n",
    "            # Compute the gradients and update the weights\n",
    "\n",
    "            derror_dbias, derror_dweights = self._compute_gradients(\n",
    "\n",
    "                input_vector, target\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "            self._update_parameters(derror_dbias, derror_dweights)\n",
    "\n",
    "\n",
    "            # Measure the cumulative error for all the instances\n",
    "\n",
    "            if current_iteration % 100 == 0:\n",
    "\n",
    "                cumulative_error = 0\n",
    "\n",
    "                # Loop through all the instances to measure the error\n",
    "\n",
    "                for data_instance_index in range(len(input_vectors)):\n",
    "\n",
    "                    data_point = input_vectors[data_instance_index]\n",
    "\n",
    "                    target = targets[data_instance_index]\n",
    "\n",
    "\n",
    "                    prediction = self.predict(data_point)\n",
    "\n",
    "                    error = np.square(prediction - target)\n",
    "\n",
    "\n",
    "                    cumulative_error = cumulative_error + error\n",
    "\n",
    "                cumulative_errors.append(cumulative_error)\n",
    "\n",
    "\n",
    "        return cumulative_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (42,) (17,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nn \u001b[39m=\u001b[39m NeuralNetwork(train_features\u001b[39m.\u001b[39mshape,\u001b[39m0.1\u001b[39m)  \u001b[39m# Adjusting n_features and n_output\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_features\u001b[39m.\u001b[39mshape\n\u001b[0;32m----> 3\u001b[0m tr_error \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mtrain(train_features, train_labels, iterations\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mplot(tr_error)\n",
      "Cell \u001b[0;32mIn[57], line 63\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, input_vectors, targets, iterations)\u001b[0m\n\u001b[1;32m     58\u001b[0m target \u001b[39m=\u001b[39m targets[random_data_index]\n\u001b[1;32m     61\u001b[0m \u001b[39m# Compute the gradients and update the weights\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m derror_dbias, derror_dweights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_gradients(\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m     input_vector, target\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_parameters(derror_dbias, derror_dweights)\n\u001b[1;32m     73\u001b[0m \u001b[39m# Measure the cumulative error for all the instances\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 25\u001b[0m, in \u001b[0;36mNeuralNetwork._compute_gradients\u001b[0;34m(self, input_vector, target)\u001b[0m\n\u001b[1;32m     22\u001b[0m layer_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sigmoid(layer_1)\n\u001b[1;32m     23\u001b[0m prediction \u001b[39m=\u001b[39m layer_2\n\u001b[0;32m---> 25\u001b[0m derror_dprediction \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (prediction \u001b[39m-\u001b[39;49m target)\n\u001b[1;32m     26\u001b[0m dprediction_dlayer1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sigmoid_deriv(layer_1)\n\u001b[1;32m     27\u001b[0m dlayer1_dbias \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (42,) (17,) "
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(train_features.shape,0.1)  # Adjusting n_features and n_output\n",
    "\n",
    "tr_error = nn.train(train_features, train_labels, iterations=10000)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(tr_error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:  (36, 59)\n",
      "Dataset:\n",
      "    X_left_foot_avg  X_left_foot_var  X_right_foot_avg  X_right_foot_var  \\\n",
      "0          0.007774     7.058829e-05          0.016240      2.744935e-04   \n",
      "1          0.011890     1.629459e-04          0.020960      6.396525e-04   \n",
      "2          0.196221     9.350937e-02          1.767068      3.546706e+00   \n",
      "3          0.153154     3.788091e-02          0.572844      6.323509e-01   \n",
      "4          0.003769     2.251460e-05          0.005164      3.626099e-05   \n",
      "5          0.007820     6.452291e-05          0.389714      1.181945e-01   \n",
      "6          0.000617     2.500998e-07          0.000552      1.632158e-07   \n",
      "7          0.001831     2.373220e-06          0.003056      1.134762e-05   \n",
      "8          0.007563     4.565466e-05          0.295923      5.387698e-01   \n",
      "9          0.029616     8.773440e-04          0.016128      3.522086e-04   \n",
      "10         0.010689     9.610476e-04          0.003018      1.625743e-05   \n",
      "11         0.096384     3.785791e-02          0.061323      1.576121e-02   \n",
      "12         0.956324     3.607086e-01          0.017002      1.589019e-04   \n",
      "13         0.864811     4.084891e-01          0.013072      6.151609e-05   \n",
      "14         0.183718     2.168703e-01          0.013575      6.666581e-05   \n",
      "15         0.024537     1.864654e-04          0.080096      7.566771e-03   \n",
      "16         0.076695     1.119438e-02          0.111322      5.237802e-02   \n",
      "17         1.575408     3.148357e+00          1.834279      3.319642e+00   \n",
      "18         0.244465     2.840290e-01          0.199117      2.003225e-01   \n",
      "19         0.095791     4.451912e-02          0.081393      3.557044e-02   \n",
      "20         0.004306     8.368964e-06          0.004454      6.411156e-06   \n",
      "21         0.006251     1.476073e-05          0.005595      2.145821e-05   \n",
      "22         0.075290     3.632244e-02          0.003636      1.355318e-05   \n",
      "23         0.003370     4.710887e-06          0.003778      1.238867e-05   \n",
      "24         0.074936     2.326622e-02          0.085192      1.570855e-02   \n",
      "25         0.113839     9.319365e-02          0.059432      2.602958e-02   \n",
      "26         0.041112     2.125591e-03          0.125364      1.648261e-02   \n",
      "27         0.173227     6.746270e-02          0.106368      5.324409e-02   \n",
      "28         0.259906     1.830595e-01          0.396923      3.678332e-01   \n",
      "29         0.093346     1.499635e-02          0.225583      7.728023e-02   \n",
      "30         0.226213     3.863422e-02          0.652288      9.474454e-01   \n",
      "31         0.232010     7.905337e-02          0.144011      8.960493e-02   \n",
      "32         0.193255     5.805701e-02          0.406455      6.345223e-01   \n",
      "33         0.218916     7.092391e-02          0.176296      1.084080e-01   \n",
      "34         0.229296     1.855405e-01          0.409436      4.783079e-01   \n",
      "35         0.167550     4.765321e-02          0.243933      1.822869e-01   \n",
      "\n",
      "    X_left_ankle_avg  X_left_ankle_var  X_right_ankle_avg  X_right_ankle_var  \\\n",
      "0           0.027433          0.000534           0.052894       1.818841e-03   \n",
      "1           0.036153          0.001080           0.067759       4.399652e-03   \n",
      "2           0.124725          0.018970           2.213771       3.007082e+00   \n",
      "3           0.168056          0.048041           0.495058       3.406924e-01   \n",
      "4           0.007432          0.000034           0.009989       1.127561e-04   \n",
      "5           0.012321          0.000108           0.422730       7.520250e-02   \n",
      "6           0.001986          0.000020           0.001373       8.440537e-07   \n",
      "7           0.161436          0.544280           0.004217       3.888733e-05   \n",
      "8           0.199153          0.630415           0.338642       4.455530e-01   \n",
      "9           0.057657          0.001865           0.034971       1.311301e-03   \n",
      "10          0.023991          0.000904           0.010839       1.344555e-04   \n",
      "11          0.104674          0.020945           0.066355       1.055812e-02   \n",
      "12          0.785962          0.203365           0.054883       1.047679e-03   \n",
      "13          0.795778          0.359782           0.037922       5.112425e-04   \n",
      "14          0.190451          0.153201           0.040785       7.443437e-04   \n",
      "15          0.064818          0.001333           0.099971       3.440135e-03   \n",
      "16          0.115099          0.010196           0.135809       5.647245e-02   \n",
      "17          1.596399          2.548712           1.824750       2.860149e+00   \n",
      "18          0.262340          0.278853           0.230161       2.158382e-01   \n",
      "19          0.114010          0.043757           0.096333       3.487948e-02   \n",
      "20          0.011073          0.000046           0.010207       3.446378e-05   \n",
      "21          0.012710          0.000079           0.010735       6.942536e-05   \n",
      "22          0.082398          0.038471           0.007585       5.386516e-05   \n",
      "23          0.007022          0.000023           0.006897       3.048240e-05   \n",
      "24          0.091578          0.020803           0.106813       1.443329e-02   \n",
      "25          0.130657          0.072482           0.080275       2.609476e-02   \n",
      "26          0.066951          0.005983           0.178484       2.010984e-02   \n",
      "27          0.166465          0.047026           0.121470       3.977219e-02   \n",
      "28          0.276345          0.139623           0.426408       3.316416e-01   \n",
      "29          0.148081          0.019278           0.261537       6.573989e-02   \n",
      "30          0.296287          0.050122           0.692148       8.415941e-01   \n",
      "31          0.280310          0.042416           0.162194       6.173159e-02   \n",
      "32          0.281107          0.072290           0.456275       6.077440e-01   \n",
      "33          0.252319          0.071452           0.239795       1.216095e-01   \n",
      "34          0.285403          0.154257           1.059947       1.184002e+01   \n",
      "35          0.266950          0.101327           0.273718       1.497125e-01   \n",
      "\n",
      "    X_left_knee_avg  X_left_knee_var  ...  Y_OneHot_spine-Cshoulder  \\\n",
      "0          0.089811         0.007160  ...                     False   \n",
      "1          0.144923         0.009984  ...                      True   \n",
      "2          0.258535         0.042587  ...                     False   \n",
      "3          0.278883         0.055658  ...                     False   \n",
      "4          0.038700         0.000678  ...                     False   \n",
      "5          0.096670         0.004867  ...                     False   \n",
      "6          0.007852         0.000026  ...                     False   \n",
      "7          0.019690         0.000283  ...                     False   \n",
      "8          0.093191         0.005855  ...                     False   \n",
      "9          0.127043         0.004649  ...                     False   \n",
      "10         0.056476         0.006010  ...                     False   \n",
      "11         0.111586         0.003972  ...                     False   \n",
      "12         0.505988         0.073485  ...                     False   \n",
      "13         0.796631         0.283027  ...                     False   \n",
      "14         0.273419         0.095778  ...                     False   \n",
      "15         0.225941         0.017482  ...                      True   \n",
      "16         0.374687         0.070341  ...                     False   \n",
      "17         1.606253         0.878016  ...                     False   \n",
      "18         0.243975         0.103690  ...                     False   \n",
      "19         0.227029         0.022935  ...                      True   \n",
      "20         0.058118         0.002212  ...                     False   \n",
      "21         0.056635         0.004083  ...                     False   \n",
      "22         0.071614         0.014438  ...                     False   \n",
      "23         0.036964         0.000369  ...                     False   \n",
      "24         0.190341         0.012735  ...                     False   \n",
      "25         0.229477         0.105162  ...                      True   \n",
      "26         0.266724         0.022354  ...                     False   \n",
      "27         0.273103         0.029796  ...                     False   \n",
      "28         0.356285         0.051648  ...                     False   \n",
      "29         0.320806         0.025919  ...                      True   \n",
      "30         0.340015         0.022841  ...                      True   \n",
      "31         0.333476         0.034617  ...                      True   \n",
      "32         0.404080         0.046130  ...                     False   \n",
      "33         0.323282         0.057046  ...                     False   \n",
      "34         0.624321         2.676595  ...                      True   \n",
      "35         0.331654         0.069676  ...                     False   \n",
      "\n",
      "    Y_OneHot_Lhand-Lwrist  Y_OneHot_Rhand-Rwrist  Y_OneHot_Lwrist-Lelbow  \\\n",
      "0                   False                  False                   False   \n",
      "1                   False                  False                   False   \n",
      "2                   False                  False                   False   \n",
      "3                   False                  False                   False   \n",
      "4                   False                  False                   False   \n",
      "5                   False                  False                   False   \n",
      "6                   False                  False                   False   \n",
      "7                   False                  False                   False   \n",
      "8                   False                  False                   False   \n",
      "9                   False                  False                   False   \n",
      "10                  False                   True                   False   \n",
      "11                  False                  False                   False   \n",
      "12                  False                  False                   False   \n",
      "13                  False                  False                   False   \n",
      "14                  False                  False                    True   \n",
      "15                  False                  False                   False   \n",
      "16                  False                  False                   False   \n",
      "17                  False                  False                   False   \n",
      "18                  False                  False                   False   \n",
      "19                  False                  False                   False   \n",
      "20                  False                  False                   False   \n",
      "21                  False                  False                   False   \n",
      "22                  False                  False                   False   \n",
      "23                  False                  False                   False   \n",
      "24                  False                  False                   False   \n",
      "25                  False                  False                   False   \n",
      "26                  False                  False                   False   \n",
      "27                  False                  False                   False   \n",
      "28                  False                  False                   False   \n",
      "29                  False                  False                   False   \n",
      "30                  False                  False                   False   \n",
      "31                  False                  False                   False   \n",
      "32                  False                  False                   False   \n",
      "33                  False                  False                   False   \n",
      "34                  False                  False                   False   \n",
      "35                  False                  False                   False   \n",
      "\n",
      "    Y_OneHot_Rwrist-Relbow  Y_OneHot_Lelbow-Lshoulder  \\\n",
      "0                    False                      False   \n",
      "1                    False                      False   \n",
      "2                    False                      False   \n",
      "3                    False                      False   \n",
      "4                    False                      False   \n",
      "5                    False                      False   \n",
      "6                    False                      False   \n",
      "7                    False                      False   \n",
      "8                    False                      False   \n",
      "9                    False                      False   \n",
      "10                   False                      False   \n",
      "11                   False                      False   \n",
      "12                   False                      False   \n",
      "13                   False                      False   \n",
      "14                   False                       True   \n",
      "15                   False                      False   \n",
      "16                   False                      False   \n",
      "17                   False                      False   \n",
      "18                   False                      False   \n",
      "19                   False                      False   \n",
      "20                   False                      False   \n",
      "21                   False                       True   \n",
      "22                   False                       True   \n",
      "23                    True                      False   \n",
      "24                   False                      False   \n",
      "25                   False                      False   \n",
      "26                   False                       True   \n",
      "27                   False                      False   \n",
      "28                   False                      False   \n",
      "29                   False                      False   \n",
      "30                   False                      False   \n",
      "31                   False                      False   \n",
      "32                   False                       True   \n",
      "33                   False                      False   \n",
      "34                   False                      False   \n",
      "35                   False                      False   \n",
      "\n",
      "    Y_OneHot_Relbow-Rshoulder  Y_OneHot_Lshoulder-Cshoulder  \\\n",
      "0                       False                         False   \n",
      "1                       False                         False   \n",
      "2                       False                         False   \n",
      "3                       False                          True   \n",
      "4                       False                         False   \n",
      "5                       False                         False   \n",
      "6                       False                          True   \n",
      "7                       False                         False   \n",
      "8                       False                         False   \n",
      "9                       False                          True   \n",
      "10                      False                         False   \n",
      "11                       True                         False   \n",
      "12                      False                         False   \n",
      "13                      False                         False   \n",
      "14                      False                         False   \n",
      "15                      False                         False   \n",
      "16                      False                         False   \n",
      "17                      False                         False   \n",
      "18                      False                         False   \n",
      "19                      False                         False   \n",
      "20                       True                         False   \n",
      "21                      False                          True   \n",
      "22                      False                          True   \n",
      "23                      False                         False   \n",
      "24                      False                         False   \n",
      "25                      False                         False   \n",
      "26                      False                          True   \n",
      "27                      False                         False   \n",
      "28                      False                         False   \n",
      "29                      False                         False   \n",
      "30                      False                         False   \n",
      "31                       True                         False   \n",
      "32                       True                          True   \n",
      "33                      False                         False   \n",
      "34                      False                         False   \n",
      "35                       True                         False   \n",
      "\n",
      "    Y_OneHot_Rshoulder-Cshoulder  Y_OneHot_Cshoulder-head  \n",
      "0                           True                    False  \n",
      "1                          False                     True  \n",
      "2                          False                    False  \n",
      "3                          False                    False  \n",
      "4                          False                     True  \n",
      "5                          False                    False  \n",
      "6                          False                    False  \n",
      "7                           True                    False  \n",
      "8                          False                    False  \n",
      "9                          False                    False  \n",
      "10                         False                    False  \n",
      "11                          True                     True  \n",
      "12                         False                    False  \n",
      "13                         False                    False  \n",
      "14                         False                    False  \n",
      "15                         False                    False  \n",
      "16                         False                    False  \n",
      "17                         False                    False  \n",
      "18                         False                    False  \n",
      "19                         False                    False  \n",
      "20                         False                    False  \n",
      "21                         False                    False  \n",
      "22                         False                    False  \n",
      "23                         False                    False  \n",
      "24                         False                    False  \n",
      "25                         False                    False  \n",
      "26                         False                    False  \n",
      "27                         False                    False  \n",
      "28                         False                    False  \n",
      "29                         False                    False  \n",
      "30                         False                    False  \n",
      "31                          True                    False  \n",
      "32                          True                    False  \n",
      "33                         False                    False  \n",
      "34                          True                    False  \n",
      "35                         False                    False  \n",
      "\n",
      "[36 rows x 59 columns]\n",
      "[[0.03632243939478563 0.003636397704343119 1.3553181554372895e-05]\n",
      " [2.500997865292631e-07 0.0005518025715990722 1.632157961885952e-07]\n",
      " [0.4084890661541352 0.013072094496807527 6.151608919737395e-05]\n",
      " [0.0008773440383005301 0.01612766470287185 0.00035220861473453813]\n",
      " [0.002125590550669405 0.12536368822589625 0.016482609928784125]\n",
      " [0.18305945293126655 0.3969231588706177 0.3678332165129261]\n",
      " [0.03785791022536891 0.06132283556918652 0.0157612130007352]\n",
      " [2.2514601739751853e-05 0.005164401785319631 3.626098615660662e-05]\n",
      " [0.07905337037448938 0.1440105319121541 0.08960492810203764]\n",
      " [0.03863421885965227 0.6522883408895498 0.9474454184306205]\n",
      " [3.1483568784714406 1.834279218432984 3.319642479484461]\n",
      " [0.18554051105962582 0.40943573450513027 0.4783078616039606]\n",
      " [0.21687034841533567 0.013574883964236478 6.666580690818244e-05]\n",
      " [0.09319364774382055 0.05943194230552612 0.026029576229851026]\n",
      " [0.06746270402588447 0.10636819580900814 0.05324408744654035]\n",
      " [1.4760733799083179e-05 0.005595488859118961 2.1458214432029774e-05]\n",
      " [0.09350936837975064 1.767067609706617 3.5467062871207875]\n",
      " [8.368963757958157e-06 0.004454067134291927 6.411155715721011e-06]\n",
      " [0.0009610476346042188 0.0030177114916862236 1.625742566057754e-05]\n",
      " [0.011194380276598949 0.1113216419267225 0.05237801880007552]\n",
      " [0.0001864654170980864 0.08009622039160721 0.007566771159110545]\n",
      " [4.710887240025667e-06 0.003778478997603108 1.2388667464343182e-05]\n",
      " [0.03788090611069245 0.5728438724303467 0.6323509320400804]\n",
      " [0.023266220162994032 0.08519177514522301 0.015708550748076935]\n",
      " [4.5654661130435744e-05 0.2959225087032186 0.5387697605238587]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (150) does not match length of index (36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m random\u001b[39m.\u001b[39mshuffle(data_train)\n\u001b[1;32m     38\u001b[0m \u001b[39m#Creat a column to represent for Category of Data and a column to represent for Data in Train or Test\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m balance_data[\u001b[39m'\u001b[39;49m\u001b[39mCategory\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mCategorical\u001b[39m.\u001b[39mfrom_codes(iris\u001b[39m.\u001b[39mtarget,iris\u001b[39m.\u001b[39mtarget_names)\n\u001b[1;32m     40\u001b[0m balance_data[\u001b[39m'\u001b[39m\u001b[39mInfor\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data_train\n\u001b[1;32m     41\u001b[0m \u001b[39m#Train data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3947\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3948\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3949\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3950\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4135\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4142\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4143\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4145\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4146\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4147\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4148\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4149\u001b[0m     ):\n\u001b[1;32m   4150\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4151\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4870\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4867\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4869\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4870\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4871\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (150) does not match length of index (36)"
     ]
    }
   ],
   "source": [
    "#TRAN QUANG DAT - 14520156\n",
    "#created in December 15 2017\n",
    "#MultiLabel-DecicisionTree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "import random\n",
    "import math\n",
    "\n",
    "#Load Iris Data from sklearn\n",
    "iris = load_iris()\n",
    "#Use Pandas method to import data into pandas DataFrame, save them in balanced_data\n",
    "balance_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "balance_data=dataset\n",
    "#Check the length and dimension of DataFrame\n",
    "lengthData = len(balance_data)\n",
    "print (\"Dataset Shape: \",balance_data.shape )\n",
    "print (\"Dataset:\")\n",
    "print (balance_data)\n",
    "#Split data into train and test\n",
    "X = balance_data.values[:, 1:4]\n",
    "Y = balance_data.values[:,0]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)\n",
    "print(X_train)\n",
    "#Devide dataset into train and test with 7:3\n",
    "numof_data_train = math.ceil(lengthData*0.7)\n",
    "numof_data_test = math.floor(lengthData*0.3)\n",
    "#Determine data_train by 1 and data_test by 0\n",
    "data_train = [1 for i in range(0,numof_data_train)]\n",
    "data_train.extend([0 for i in range(0,numof_data_test)])\n",
    "#Use random Shuffe to mix data\n",
    "random.shuffle(data_train)\n",
    "#Creat a column to represent for Category of Data and a column to represent for Data in Train or Test\n",
    "balance_data['Category'] = pd.Categorical.from_codes(iris.target,iris.target_names)\n",
    "balance_data['Infor'] = data_train\n",
    "#Train data\n",
    "train, test = balance_data[balance_data['Infor']==1], balance_data[balance_data['Infor']==0]\n",
    "real_labels = pd.factorize(test['Category'])[0]\n",
    "y = pd.factorize(train['Category'])[0]\n",
    "x = train[balance_data.columns[:4]]\n",
    "#Apply Decision Tree of Sklearn\n",
    "clf = DecisionTreeClassifier(random_state=100)\n",
    "clf = clf.fit(x,y)\n",
    "#Test\n",
    "predicted_labels = clf.predict(test[balance_data.columns[:4]])\n",
    "#Compare real and test\n",
    "print(\"Real labels: \")\n",
    "print(real_labels)\n",
    "print(\"Predicted labels: \")\n",
    "print(predicted_labels)\n",
    "#Apply Accuracy_score for checking accuracy\n",
    "print (\"Accuracy is\", accuracy_score(real_labels,predicted_labels)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n",
      "Accuracy is 0.0\n",
      "Average Accuracy of 10 tests: 0.0\n"
     ]
    }
   ],
   "source": [
    "#TRAN QUANG DAT - 14520156\n",
    "#created in December 15 2017\n",
    "#MultiLabel-DecicisionTree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#find Average Accuracy\n",
    "total = 0\n",
    "for i in range(0,10):\n",
    "    #Apply Random Forest of Sklearn\n",
    "    clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    #Test\n",
    "    predicted_labels = clf.predict(X_test)\n",
    "    #Compare real and test\n",
    "    #Apply Accuracy_score for checking accuracy\n",
    "\n",
    "    temp = accuracy_score(y_test,predicted_labels)*100\n",
    "    total = total + temp\n",
    "    print (\"Accuracy is\", temp)   \n",
    "    print (\"Average Accuracy of 10 tests:\", total/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
