\section{Spectral Clustering}
Clusterization aims to identify groups of nodes that exhibit significant similarity based on the features or data they represent.
This technique is often employed when data is represented as vectors and you want to uncover similarity relationships among these vectors.\\
The Shi-Malik algorithm is a clustering technique based on analyzing the eigenvalues and eigenvectors of the Laplacian matrix of a graph constructed from data.\\
This type of algorithm can be used to partition data into clusters based on their connectivity structure.
Clusterization works to find natural divisions in the graph so that nodes within each cluster are similar to each other based on cosine similarity. \\
The objective is to unveil hidden structures within data so that similar objects are grouped together to aid in data understanding and analysis.
The algorithm has some limitations due to the initialization phase, such as sensitivity to the initial choice of centroids and the requirement to specify the number of clusters beforehand.\\
This is why there will be a subsequent phase of cluster stabilization over time.

\subsection{Shi-Malik Algorithm}
The operation of the Shi-Malik Algorithm can be summarized in these 8 steps:

\begin{enumerate}
    
  \item \textbf{Calculation of Laplacian Matrix:}
  The Laplacian matrix is the difference between the diagonal matrix of node degrees \(D\) and the similarity matrix \(A\): 

  \begin{equation}
    L = D - A
  \end{equation}
  
  \item \textbf{Calculation of Laplacian's Eigenvectors and Eigenvalues:}
  Eigenvectors and eigenvalues of the Laplacian matrix are calculated using the $\texttt{np.linalg.eig()}$ function. Eigenvectors represent new transformed data features in the eigenvector space, while eigenvalues provide information about the overall variance of data along the corresponding eigenvectors.
  
  \item \textbf{Selection of Eigenvector to use:}
  Eigenvectors are ordered based on their associated eigenvalues. \\
  The eigenvector corresponding to the second-largest eigenvalue is selected (the first eigenvalue corresponds to a constant eigenvector representing scale and is ignored).
  
  \item \textbf{Binarization of Eigenvector:}
  The selected eigenvector is binarized by assigning the value 1 to data points with values greater than 0 in the eigenvector and 0 otherwise. \\
  This process separates the data into two initial groups.
  
  \item \textbf{Cluster Splitting:}
  A recursive cluster splitting algorithm, known as \texttt{split\_a\_cluster}, is applied to further divide clusters into smaller subclusters.
  The option \textit{ncut} is selected as optimization options.
  The sum of weights of data points within the two subclusters is calculated, and the optimal cutting point that minimizes the weight cut is identified.
  The resulting two subclusters are treated separately, and the splitting algorithm is applied to each of them.
  
  \item \textbf{Iterative Cluster Splitting:}
  The splitting algorithm is applied iteratively until a sufficient number of clusters are created to satisfy the desired cluster count $k$.
  At each iteration, points to be split are selected based on the weights of the current clusters.
  The eigenvector associated with each subcluster is computed and used for the next iteration.
  
  \item \textbf{Final Cluster Assignment:}
  At the end of the algorithm, data points are assigned to clusters based on their membership in the obtained subclusters.
  
  \item \textbf{Returning Results:}
  The output of the algorithm is an array of cluster assignments, where each data point is associated with a cluster number.
  
\end{enumerate}

In essence, the algorithm starts by dividing data into two initial clusters, then proceeds with iteratively splitting smaller clusters until the desired number of clusters is achieved.
This method leverages the spectral information of the data graph to obtain a data partition into clusters.


\subsection{Further Optimization}
The order in which the various clusters are created and chosen is random, and therefore, this randomness can lead to complications in terms of visualization. Hence, there is a need for cluster stabilization over time as a subsequent step. This issue is due to various factors, including:

\begin{enumerate}

    \item \textbf{Eigenvector Sensitivity:}
    The algorithm relies on utilizing the eigenvectors of the Laplacian matrix to define new data features. Eigenvectors can exhibit slight variations in different iterations, especially when eigenvalues are very close to each other. These variations can impact the separation of data into clusters.
    
    \item \textbf{Different Initialization:}
    The initialization of the cluster splitting process can vary across iterations. Since the algorithm begins by dividing data into two clusters, minor differences in the initial choices of points to split can propagate through subsequent iterations and lead to different outcomes.
    
    \item \textbf{Recursive Splitting Process:}
    The recursive cluster splitting algorithm might generate different subclusters in various iterations due to variations in point weights or choices of optimal splitting points. These variations can result in slightly different data partitions.

\end{enumerate}    